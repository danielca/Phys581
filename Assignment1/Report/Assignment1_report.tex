\documentclass[twocolumn]{myarticle}

\usepackage{mymacros}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{booktabs}

\lstset{%
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
stepnumber=1,}

\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\d}{\mathrm{d}}

\begin{document}

\title{Physics 581, Assignment 1:\\Monte Carlo methods in computational physics}
\author{Casey Daniel and Chris Deimert}
\date{\today}

\maketitle

\section{Introduction}
\label{sec:introduction}

In this report, we explore a number of Monte Carlo numerical methods.
Monte Carlo methods use pseudorandom numbers to explore complicated systems.
These methods tend to converge slowly for simple problems, but can be very efficient for complex problems: especially those with a large number of variables.

\section{Random number generators}
\label{sec:random_number_generators}

\subsection{Pseudorandom numbers}
\label{subsec:pseudorandom_numbers}

First, we explore the generation of pseudo-random sequences of numbers.
These are key in any Monte Carlo method.

Most often, random number generators try to generate samples of a uniform random variable $ U $, whose probability distribution function is
\begin{align}
    f_U(u) &= \begin{cases} 1 & \text{if } 0 \leq u < 1 \\ 0 & \text{otherwise} \end{cases}
\end{align}
The mean of $ U $ is
\begin{align}
    \mu &= \int_{0}^{1} u \, \d u = \frac{1}{2}.
\end{align}
The variance is
\begin{align}
    \sigma^2 &= \int_0^1 \left(u - \mu\right)^2 \, \d u = \int_{-1/2}^{1/2} u'^2 \, \d u' = \frac{1}{12},
\end{align}
giving standard deviation of
\begin{align}
    \sigma &= \frac{1}{\sqrt{12}}.
\end{align}

We now explore some implementations of uniform random number generators, given in Press, Teukolsky, and Vetterling's ``Numerical Recipes.''
These are implemented in the random numbers module of Section~\ref{subsec:random_numbers_module_code}.
The first, \texttt{ran0}, implements a simple linear congruential method (LCM) generator:
\begin{align}
    I_{j+1} &= a I_j \mod m
\end{align}
where $ a = 7^5 $ and $ m = 2^{31}-1 $.
As a practical matter (avoiding overflow), this is implemented using
\begin{align}
I_{j+1} &= \begin{cases} a\left(I_j \mod q\right) - r[z/q] & \text{if it is} \geq 0 \\ a\left( I_j \mod q \right) - r[z/q] + m & \text{otherwise} \end{cases}
\end{align}
where $ a = 7^5 $, $ m = 2^{31} - 1 $, $ q = 127773 $, and $ r = 2836 $.
This generator is suggested as a baseline: any random number generator which does not perform at least as well as this one should probably not be used for serious work.
The \texttt{ran1} function uses the same LCM formula as \texttt{ran0}, but it also shuffles the resulting sequence around.
This helps to eliminate certain weaknesses which \texttt{ran0} is known to have.

The \texttt{ran2} generator combines two (LCM) generators with parameters $ m_1 = 2147483563 $, $ a_1 = 40014 $, $ q_1 = 53668 $, $ r_1 = 12211 $, $ m_2 = 2147483399 $, $ a_2 = 40692 $, $ q_2 = 52774 $, $ r_2 = 3791 $.
By combining two random generators together, we obtain a generator with a period of around $ 10^{18} $ which is a ``perfect'' random number generator for most practical purposes.

For all of the above LCM methods, the upper end point is never returned.
This is because of the use of $ (\mod m )$, which guarantees that all numbers $ I_j $ are at most $ m - 1 $.
Then the largest number returned is
\begin{align}
    \frac{m-1}{m} &< 1
\end{align}

The final generator, \texttt{ran3}, is from Donald Knuth, and it uses a subtractive method rather than an LCM method.
These methods use
\begin{align}
    I_j &= \left( I_{j-k} - I_{j-l} \right) \mod m
\end{align}
with fixed $ k $, $ l $, and $ m $.
Again, the upper end point can never be returned because of the $ (\mod m) $.

Each of these random number generators was tested with different numbers of iterations.
The gfortran intrinsic random generator is also included for comparison.
The errors in the mean and standard deviation of the resulting sequences compared to the theoretical mean and standard deviation are plotted in Figures~\ref{fig:random_generators_mean} and \ref{fig:random_generators_std_dev}.
It is seen that \texttt{ran0} deviates the furthest from the expected theoretical mean and standard deviation.
The others are roughly equivalent with the gfortran generator doing slightly better.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Random_generators_mean.pdf}
    \caption{%
        Absolute difference between the means of pseudo-random sequences and the theoretical mean $ (1/2) $.
        Smaller values as sample size increases indicates a better random number generator.
    }
    \label{fig:random_generators_mean}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Random_generators_std_dev.pdf}
    \caption{%
        Absolute difference between the standard deviations of pseudo-random sequences and the theoretical standard deviation $ (1/\sqrt{12}) $.
        Smaller values as sample size increases indicates a better random number generator.
    }
    \label{fig:random_generators_std_dev}
    \end{center}
\end{figure}

As another test, the autocorrelation function was calculated for the sequences of $ 10^6 $ numbers.
This is plotted in Figure~\ref{fig:random_generators_auto_corr}.
The \texttt{ran0} function again performs the worst: its autocorrelation is higher on average, and we can see peaks in the autocorrelation which indicate that certain elements of the sequence are strongly correlated to each other.
It is difficult to see the plots of the other three functions because of the sheer density of points.
It is important to plot with this density, though, because otherwise important features like the peaks in the \texttt{ran0} plot may be lost.
The other functions perform about the same in terms of autocorrelation: better than \texttt{ran0}, with lower autocorrelations on average and no major peaks.
All of the functions see an increase in the autocorrelation for higher values of $ k $.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Random_generators_auto_corr.png}
    \caption{%
        Absolute value of the autocorrelation function for each random sequence.
        A truly random sequence should have an autocorrelation of close to zero for every $ k $.
    }
    \label{fig:random_generators_auto_corr}
    \end{center}
\end{figure}

\subsection{White noise from random numbers}
\label{subsec:white_noise_from_random_numbers}

The distribution for a uniform random variable $ X $ is given by
\begin{align}
    f_X(x) &= \begin{cases} \frac{1}{b-a} & \text{for } a \leq x \leq b \\ 0 & \text{otherwise} \end{cases}
\end{align}
It is easy to show that this distribution has expected value $ \left\langle X \right\rangle = \frac{1}{2} (a+b) $ and variance $ (\Delta X)^2 = \frac{1}{12} (b-a)^2 $.
We can obtain zero mean and unit variance by setting
\begin{align}
    a = -\sqrt{3} ; \qquad b = \sqrt{3}
\end{align}
In terms of the random variable $ U $ which is uniformly distributed on $ [0, 1) $, $ X $ is given by:
\begin{align}
    X &= 2 \sqrt{3} U - \sqrt{3}
\end{align}

An example of pseudo-randomly generated white-noise is plotted in Figure~\ref{fig:white_noise}, along with its autocorrelation.
As expected, the white noise is centred around zero, and most of the values are between $ -1 $ and $ 1 $.
Also, as expected, the autocorrelation is quite small: between $ 10^{-3} $ and $ 10^{-1} $.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/White_noise.pdf}
    \caption{%
        Sequence of 100 uniform pseudorandom numbers simulating white noise, along with their autocorrelation.
        These were generated with the gfortran random number routine.
    }
    \label{fig:white_noise}
    \end{center}
\end{figure}

In the definition of white noise, there is no requirement in terms of what distribution is used, as long as the mean is zero and the variance is unity.
The Gaussian distribution given by
\begin{align}
    f_X(x) &= \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
\end{align}
satisfies these conditions, and thus it can be used to generate white noise.
This kind of Gaussian white noise is seen in Figure~\ref{fig:white_noise_gaussian}.
Again, the noise is centred around zero, most of the values are between $ -1 $ and $ 1 $, and the autocorrelation is small, as expected.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/White_noise_gaussian.pdf}
    \caption{%
        Sequence of 100 Gaussian pseudorandom numbers simulating white noise, along with their autocorrelation.
        These were generated with Dr Ouyed's random Gaussian number routine.
    }
    \label{fig:white_noise_gaussian}
    \end{center}
\end{figure}

\section{PDFs and Monte Carlo}
\label{sec:pdfs_and_monte_carlo}

A probability distribution function (PDF) must integrate to 1, so the distribution
\begin{align}
    f(x) &= cx \quad \text{on } [0, 1)
\end{align}
must have
\begin{align}
    1 &= \int_{0}^{1} cx \, \d x = \frac{c}{2} \quad \Longrightarrow \quad c = 2
\end{align}

The expected value of a random variable $ X $ with PDF $ f_X(x) $ is
\begin{align}
    E[X] &= \int x f_X(x) \, \d x
\end{align}
and the variance is
\begin{align}
    \text{Var}[X] &= E[X^2] - (E[X])^2
\end{align}

For example, the expected value of a random variable $ X $ with distribution $ f_X(x) = \lambda e^{-\lambda x} $ is
\begin{align}
    E[X] &= \int_{0}^{\infty} x \lambda e^{-\lambda x} \, \d x = \frac{1}{\lambda}
\end{align}
We also have
\begin{align}
    E[X^2] &= \int_{0}^{\infty} x^2 \lambda e^{-\lambda x} \, \d x = \frac{2}{\lambda^2}
\end{align}
which means that
\begin{align}
    \text{Var}[X] &= \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
\end{align}

As an example of a cumulative distribution function (CDF), consider the following table describing the fraction of maintenance checks completed in time less than or equal to $ t $ minutes:

\bigskip
\begin{center}
    \begin{tabular}{cc}
        \toprule
        $ t $ (min) & CDF, fraction completed \\
        \midrule
        0.0  & 0.0  \\
        5.0  & 0.02 \\
        10.0 & 0.08 \\
        15.0 & 0.21 \\
        20.0 & 0.38 \\
        25.0 & 0.80 \\
        30.0 & 0.90 \\
        \bottomrule
    \end{tabular}
\end{center}
\bigskip

We can see that 21\% of checks are completed in 15 minutes or less, 90\% are completed within 30 minutes or less, and 13\% are completed in between 10 and 15 minutes.

\subsection{Beta distribution}
\label{subsec:beta_distribution}

Another example of a PDF is the beta distribution.
With $ \alpha = 3 $ and $ \beta = 2 $, the beta distribution is given by
\begin{align}
    f_X(x) &= \frac{1}{B(3,2)} x^{2} (1-x)
\end{align}
Normalizing:
\begin{align}
    1 &= \int_{0}^{1} f_X(x) \, \d x = \frac{1}{12 B(3,2)} \quad \Longrightarrow B(3,2) = \frac{1}{12}
\end{align}

The CDF is given by
\begin{align}
    F_X(x) &= \int_{0}^{x} 12 x'^2 (1-x') \, \d x' = \left(4 - 3 x \right) x^3
\end{align}

Using the fundamental principle for sampling, we would need to generate a unit, uniform random number $ U $ and solve the equation
\begin{align}
    \left( 4 - 3X \right) X^3  - U &= 0
\end{align}
for $ X $ in the domain $ [0, 1) $.

Using the Newton-Raphson method to solve this equation, $ 10^5 $ samples were generated.
The results are shown in Figure~\ref{fig:beta_fundamental_princ}.
It can be seen from the scatter plot that the distribution is roughly what would be expected of a beta distribution.
The sample mean of this sequence was $ 0.6017 $, the standard deviation was $ 0.03998 $, and the runtime was $ 97.9 $ ms.
The mean and variance are close to the theoretical values of $ 0.6 $ and $ 0.04 $, respectively.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Beta_fundamental_princ.png}
    \caption{%
        Sequence of $ 10^5 $ Beta pseudorandom numbers generated with the fundamental principle.
    }
    \label{fig:beta_fundamental_princ}
    \end{center}
\end{figure}

Another way to generate a Beta sequence is using the accept/reject method.
The maximum value of $ f_X(x) $ is at
\begin{align}
    \frac{\d}{\d x} f_X(x) &= 0 
    \\
    12\left(2x - 3x^2\right) &= 0
    \\
    \left( x - \frac{2}{3} \right) x &= 0
\end{align}
So the maximum value is at $ x = 2/3 $, and it is
\begin{align}
    f_{X,\text{max}} &= 12 \left( 2/3 \right)^2 \left( 1 - 2/3 \right) = \frac{16}{9} \approx 1.7778
\end{align}
To be safe, we use the region $ 0 \leq x < 1 $ and $ 0 \leq y < 1.8 $ for the accept/reject method.

Using this region, $ 10^5 $ samples were generated, and the results are plotted in Figure~\ref{fig:beta_accept_reject}.
It can be seen from the scatter plot that the distribution is roughly what would be expected of a beta distribution.
The sample mean of this sequence was $ 0.6004 $, the standard deviation was $ 0.04001 $, and the runtime was $ 82.3 $ ms.
The mean and variance are even closer to the theoretical values than the values from the fundamental principle sequence, despite having a slightly shorter runtime.
This indicates the effectiveness of the accept/reject method over the fundamental principle method for distributions whose CDF's are not easily invertible.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Beta_accept_reject.png}
    \caption{%
        Sequence of $ 10^5 $ Beta pseudorandom numbers generated with the accept/reject method.
    }
    \label{fig:beta_accept_reject}
    \end{center}
\end{figure}

\subsection{Gaussian distribution}
\label{subsec:gaussian_distribution}

As another example of a non-trivial distribution, we use the Gaussian distribution
\begin{align}
    f_X(x) &= \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[ - \frac{(x - \mu)^2}{2 \sigma^2} \right]
\end{align}
with $ \mu = 5 $ and $ \sigma = 1.25 $.

The peak value of this distribution is
\begin{align}
    f_{X,\text{max}} &= \frac{1}{\sigma \sqrt{2 \pi}} \approx 0.319
\end{align}
So, assuming that $ f_X \approx 0 $ for $ |x - \mu| > 4 \sigma $, we take our accept/reject region to be $ 0 < x < 10 $ and $ 0 < y < 0.35 $.

A generic function, \texttt{gauss\_random}, was created which produces a Gaussian-distributed random variable with desired mean and standard deviation. 
It can be deen in Section~\ref{subsec:random_numbers_module_code}.
The results of $ 10^6 $ trials with a mean of $ 5 $ and a standard deviation of $ 1.25 $ are shown in Figure~\ref{fig:gauss_accept_reject}.
The normalized histogram lines up almost perfectly with the expected distribution, demonstrating that the accept/reject method is an effective way to produce Gaussian-distributed random numbers.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Gauss_accept_reject.pdf}
    \caption{%
        Histogram of a sequence of $ 10^6 $ Gaussian-distributed pseudorandom numbers generated with the accept/reject method.
        Histogram has been normalized so that it corresponds to a PDF.
    }
    \label{fig:gauss_accept_reject}
    \end{center}
\end{figure}

Another way to produce Gaussian-distributed random numbers is by making use of the central limit theorem.
The central limit theorem says that if $ X_i $ are independent and identically-distributed random variables with mean $ \mu $ and standard deviation $ \sigma $, then
\begin{align}
    S_N = \sum_{i=1}^N X_i
\end{align}
approaches a Gaussian-distributed random variable with mean $ N \mu $ and standard deviation $ \sqrt{N} \sigma $ in the limit of large $ N $.

In particular, if $ X_i $ are uniformly distributed on $ [a,b) $, we can solve for $ a $, $ b $, and $ N $ so that $ S_N $ has any desired mean and standard deviation.
If $ \mu_\text{des} $ is the desired mean and $ \sigma_\text{des} $ is the desired standard deviation, then
\begin{align}
    a &= \frac{\mu_\text{des}}{N} - \sqrt{\frac{3\sigma^2_\text{des}}{N}}
    \\
    b &= \frac{\mu_\text{des}}{N} + \sqrt{\frac{3\sigma^2_\text{des}}{N}}
\end{align}

So to generate a Gaussian random number of mean $ \mu_\text{des} $ and standard deviation $ \sigma_\text{des} $, we pick an $ N $: large enough that the central limit theorem applies, but not so large as to waste computation time. 
Then we generate $ N $ uniform random variables on $ [a, b) $, with $ a $ and $ b $ given above.
We add these numbers together to get the desired result.

This method was applied using $ N = 10 $, $ \mu_\text{des} = 5 $ and $ \sigma_\text{des} = 1.25 $.
A sequence of $ 10^4 $ random numbers was created, and the results are plotted in Figure~\ref{fig:gauss_central_limit}.
This histogram clearly lines up with the desired distribution, though the fit is not as clean as it was with the accept/reject method.
Of course, that comparison is unfair since this histogram was generated from a much smaller sample size.

Though this method is an interesting demonstration of the central limit theorem, it was found that it is not an efficient way to generate Gaussian random numbers compared with the accept/reject approach.
This is not surprising since at least 10 uniform random numbers must be generated for each Gaussian random number produced.
For more accurate distribution fits, even more uniform random numbers must be used in the sum.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Gauss_central_limit.pdf}
    \caption{%
        Histogram of a sequence of $ 10^4 $ Gaussian-distributed pseudorandom numbers generated with the central limit theorem method.
        Histogram has been normalized so that it corresponds to a PDF.
    }
    \label{fig:gauss_central_limit}
    \end{center}
\end{figure}

\section{Markov chain}
\label{sec:markov_chain}

\subsection{Weather example}
\label{subsec:weather_example}

Consider a classic Markov chain: the weather in a city is only sunny or cloudy, and the weather tomorrow only depends on the weather today.
Let $ X_i $ be the weather on day $ i $, where $ X_i = 1 $ corresponds to sunny and $ X_i = 0 $ corresponds to cloudy.
Then the relevant probabilities are
\begin{align}
    P[1|1] = P[X_{i+1} = 1 | X_{i} = 1] &= 0.75
    \\
    P[0|1] = P[X_{i+1} = 0 | X_{i} = 1] &= 0.25
    \\
    P[1|0] = P[X_{i+1} = 1 | X_{i} = 0] &= 0.50
    \\
    P[0|0] = P[X_{i+1} = 0 | X_{i} = 0] &= 0.50
\end{align}

The transition equation in matrix form is
\begin{align}
    \mat{P[X_{i+1} = 1] \\ P[X_{i+1} = 0]} &= \mat{P[1|1] & P[1|0] \\ P[0|1] & P[0|0]} \mat{P[X_{i} = 1] \\ P[X_{i} = 0]}
    \\
    \mat{P[X_{i+1} = 1] \\ P[X_{i+1} = 0]} &= \mat{0.75 & 0.50 \\ 0.25 & 0.50} \mat{P[X_{i} = 1] \\ P[X_{i} = 0]}
\end{align}

In the limiting case, we have
\begin{align}
    \mat{P[X_\infty = 1] \\ P[X_\infty = 0]} &= \mat{0.75 & 0.50 \\ 0.25 & 0.50} \mat{P[X_\infty = 1] \\ P[X_\infty = 0]}
    \\
    \Longrightarrow \quad \mat{0 \\ 0} &= \mat{-0.25 & 0.50 \\ 0.25 & -0.50} \mat{P[X_\infty = 1] \\ P[X_\infty = 0]}
\end{align}
Which implies that $ P[X_\infty = 1] = 2 P[X_\infty = 0] $.
Insisting that the probabilities add up to 1 gives the steady-state probabilities.
\begin{align}
    \mat{P[X_\infty = 1] \\ P[X_\infty = 0]} &= \mat{2/3 \\ 1/3}
\end{align}

Alternatively, we can find that the eigenvectors of the transition matrix are $ [2,1]^T $ and $ [-1,1]^T $ with eigenvalues $ 1 $ and $ 1/4 $, respectively.
We want the eigenvector corresponding to the eigenvalue $ 1 $: the matrix applied to the steady state probabilities should return the same state.
So we discard the eigenvector corresponding to the eigenvalue $ 1/4 $.
(This is not surprising, since that eigenvector would give negative probabilities).
Normalizing so that the probabilities add up to 1, we get the same result:
\begin{align}
    \mat{P[X_\infty = 1] \\ P[X_\infty = 0]} &= \mat{2/3 \\ 1/3}
\end{align}

A simulation of this Markov Chain was performed, with the results shown in Figure~\ref{fig:weather_simulation}.
In calculating the probability of a sunny day, the first 10 days were discarded (``burn-in'').
It was calculated that the probability of a sunny day was $0.720$, which is not quite the theoretical value of $ 0.6667 $.
However, with an increased number of trials, this value converges to the theoretical one.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Weather_simulation.pdf}
    \caption{%
        Simulation of the weather Markov Chain, with the expected and calculated probabilities of a sunny day shown for reference.
    }
    \label{fig:weather_simulation}
    \end{center}
\end{figure}

\subsection{Edged square}
\label{subsec:edged_square}

Let $ P_{n \to m} $ be the probability of moving from corner $ n $ to corner $ m $, and let $ P_n[i] $ be the probability of being on corner $ n $ on the $ i $th move.
Then the transition equation is
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1] \\ P_4[i+1]} &= 
    \mat{P_{1 \to 1} & P_{2 \to 1} & P_{3 \to 1} & P_{4 \to 1} \\
         P_{1 \to 2} & P_{2 \to 2} & P_{3 \to 2} & P_{4 \to 2} \\
         P_{1 \to 3} & P_{2 \to 3} & P_{3 \to 3} & P_{4 \to 3} \\
         P_{1 \to 4} & P_{2 \to 4} & P_{3 \to 4} & P_{4 \to 4} }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i] \\ P_4[i]} 
\end{align}
Plugging in the values, we get
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1] \\ P_4[i+1]} &= 
    \mat{0 & 1/2 & 1/3 & 1/2 \\
         1/3 & 0 & 1/3 & 0 \\
         1/3 & 1/2 & 0 & 1/2 \\
         1/3 & 0 & 1/3 & 0 }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i] \\ P_4[i]} 
\end{align}

The steady state condition is then
\begin{align}
    \mat{-1 & 1/2 & 1/3 & 1/2 \\
         1/3 & -1 & 1/3 & 0 \\
         1/3 & 1/2 & -1 & 1/2 \\
        1/3 & 0 & 1/3 & -1 }
    \mat{P_1[\infty] \\ P_2[\infty] \\ P_3[\infty] \\ P_4[\infty]} &= \mat{0 \\ 0 \\ 0 \\ 0 }
\end{align}
which has solutions
\begin{align}
    P_1[\infty] &= \frac{3}{2} P_4[\infty]
    \\
    P_2[\infty] &= P_4[\infty]
    \\
    P_3[\infty] &= \frac{3}{2} P_4[\infty]
\end{align}

With the condition that all the probabilities add up to unity, we get
\begin{align}
    P_1[\infty] &= \frac{3}{10}
    \\
    P_2[\infty] &= \frac{1}{5}
    \\
    P_3[\infty] &= \frac{3}{10}
    \\
    P_4[\infty] &= \frac{1}{5}
\end{align}

Alternatively, we can show that the eigenvectors are $ [1.5, 1, 1.5, 1]^T $, $ [-1, 1, -1, 1]^T $, $ [-1, 0, 1, 0]^T $, $ [0, -1, 0, 1]^T $ with eigenvalues $ 1 $, $ -2/3 $, $ -1/3 $, and $ 0 $, respectively.
Normalizing the eigenvector corresponding to eigenvalue 1 so that its elements add up to unity gives the result above.

\subsection{Business example}
\label{subsec:business_example}

As another example, we look at customer transitions between brands.
The transition equation is given by
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1]} &= 
    \mat{P_{1 \to 1} & P_{2 \to 1} & P_{3 \to 1} \\
         P_{1 \to 2} & P_{2 \to 2} & P_{3 \to 2} \\
         P_{1 \to 3} & P_{2 \to 3} & P_{3 \to 3} }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i] \\ P_4[i]} 
\end{align}
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1]} &= 
    \mat{0.80 & 0.03 & 0.20 \\
         0.10 & 0.95 & 0.05 \\
         0.10 & 0.02 & 0.75 }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i]} 
\end{align}

The initial state (month 1) is.
\begin{align}
    \mat{P_1[1] \\ P_2[1] \\ P_3[1]} &= \mat{0.45 \\ 0.25 \\ 0.30}
\end{align}

We find the month 3 state by applying the transition matrix twice:
\begin{align}
    \mat{P_1[3] \\ P_2[3] \\ P_3[3]} &= 
    \mat{0.80 & 0.03 & 0.20 \\
         0.10 & 0.95 & 0.05 \\
         0.10 & 0.02 & 0.75 }^2
    \mat{0.45 \\ 0.25 \\ 0.30} 
    \\
    \mat{P_1[3] \\ P_2[3] \\ P_3[3]} &= \mat{0.405925 \\ 0.339125 \\ 0.254950}
\end{align}

The steady state solution is given by the eigenvector corresponding to an eigenvalue of 1.
After normalizing this eigenvector so that the elements add up to unity, we get the steady state solution as
\begin{align}
    \mat{P_1[\infty] \\ P_2[\infty] \\ P_3[\infty]} &= \mat{0.237113 \\ 0.618556 \\ 0.144330}
\end{align}
So this model predicts that Brand 2 will take most of the market share in the long run.

Whether the market will actually approach these values depends strongly on our confidence in the model.
We have assumed that the transition rates do not change from month to month, which is probably not a valid assumption long-term.
This ignores a wide array of unexpected events which could effect market shares.

\onecolumn

\section{Code}
\label{sec:code}

\subsection{Random generators code}
\label{subsec:random_generators_code}

\lstinputlisting[breaklines]{../Random_generators.f90}
\vspace{10pt}

\subsection{Random numbers module code}
\label{subsec:random_numbers_module_code}

\lstinputlisting[breaklines]{../../Modules/Random_numbers_module.f90}
\vspace{10pt}

\subsection{Random generators plotting code}
\label{subsec:random_generators_plotting_code}

\lstinputlisting[breaklines]{../Random_generators.gp}
\vspace{10pt}

\end{document}
