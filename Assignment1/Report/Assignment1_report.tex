\documentclass[twocolumn]{myarticle}

\usepackage{mymacros}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{booktabs}

\lstset{%
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
stepnumber=1,}

\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\d}{\mathrm{d}}

\begin{document}

\title{Physics 581, Assignment 1:\\Monte Carlo methods in computational physics}
\author{Casey Daniel and Chris Deimert}
\date{\today}

\maketitle

\section{Introduction}
\label{sec:introduction}

In this report, we explore a number of Monte Carlo numerical methods.
Monte Carlo methods use pseudorandom numbers to explore complicated systems.
These methods tend to converge slowly for simple problems, but can be very efficient for complex problems: especially those with a large number of variables.

\section{Random number generators}
\label{sec:random_number_generators}

\subsection{Pseudorandom numbers}
\label{subsec:pseudorandom_numbers}

First, we explore the generation of pseudo-random sequences of numbers.
These are key in any Monte Carlo method.

Most often, random number generators try to generate samples of a uniform random variable $ U $, whose probability distribution function is
\begin{align}
    f_U(u) &= \begin{cases} 1 & \text{if } 0 \leq u < 1 \\ 0 & \text{otherwise} \end{cases}
\end{align}
The mean of $ U $ is
\begin{align}
    \mu &= \int_{0}^{1} u \, \d u = \frac{1}{2}.
\end{align}
The variance is
\begin{align}
    \sigma^2 &= \int_0^1 \left(u - \mu\right)^2 \, \d u = \int_{-1/2}^{1/2} u'^2 \, \d u' = \frac{1}{12},
\end{align}
giving standard deviation of
\begin{align}
    \sigma &= \frac{1}{\sqrt{12}}.
\end{align}

We now explore some implementations of uniform random number generators, given in Press, Teukolsky, and Vetterling's ``Numerical Recipes.''
These are implemented in the random numbers module of Section~\ref{subsec:random_numbers_module_code}.
The first, \texttt{ran0}, implements a simple linear congruential method (LCM) generator:
\begin{align}
    I_{j+1} &= a I_j \mod m
\end{align}
where $ a = 7^5 $ and $ m = 2^{31}-1 $.
As a practical matter (avoiding overflow), this is implemented using
\begin{align}
I_{j+1} &= \begin{cases} a\left(I_j \mod q\right) - r[z/q] & \text{if it is} \geq 0 \\ a\left( I_j \mod q \right) - r[z/q] + m & \text{otherwise} \end{cases}
\end{align}
where $ a = 7^5 $, $ m = 2^{31} - 1 $, $ q = 127773 $, and $ r = 2836 $.
This generator is suggested as a baseline: any random number generator which does not perform at least as well as this one should probably not be used for serious work.
The \texttt{ran1} function uses the same LCM formula as \texttt{ran0}, but it also shuffles the resulting sequence around.
This helps to eliminate certain weaknesses which \texttt{ran0} is known to have.

The \texttt{ran2} generator combines two (LCM) generators with parameters $ m_1 = 2147483563 $, $ a_1 = 40014 $, $ q_1 = 53668 $, $ r_1 = 12211 $, $ m_2 = 2147483399 $, $ a_2 = 40692 $, $ q_2 = 52774 $, $ r_2 = 3791 $.
By combining two random generators together, we obtain a generator with a period of around $ 10^{18} $ which is a ``perfect'' random number generator for most practical purposes.

For all of the above LCM methods, the upper end point is never returned.
This is because of the use of $ (\mod m )$, which guarantees that all numbers $ I_j $ are at most $ m - 1 $.
Then the largest number returned is
\begin{align}
    \frac{m-1}{m} &< 1
\end{align}

The final generator, \texttt{ran3}, is from Donald Knuth, and it uses a subtractive method rather than an LCM method.
These methods use
\begin{align}
    I_j &= \left( I_{j-k} - I_{j-l} \right) \mod m
\end{align}
with fixed $ k $, $ l $, and $ m $.
Again, the upper end point can never be returned because of the $ (\mod m) $.

Each of these random number generators was tested with different numbers of iterations.
The gfortran intrinsic random generator is also included for comparison.
The errors in the mean and standard deviation of the resulting sequences compared to the theoretical mean and standard deviation are plotted in Figures~\ref{fig:random_generators_mean} and \ref{fig:random_generators_std_dev}.
It is seen that \texttt{ran0} deviates the furthest from the expected theoretical mean and standard deviation.
The others are roughly equivalent with the gfortran generator doing slightly better.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Random_generators_mean.pdf}
    \caption{%
        Absolute difference between the means of pseudo-random sequences and the theoretical mean $ (1/2) $.
        Smaller values as sample size increases indicates a better random number generator.
    }
    \label{fig:random_generators_mean}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Random_generators_std_dev.pdf}
    \caption{%
        Absolute difference between the standard deviations of pseudo-random sequences and the theoretical standard deviation $ (1/\sqrt{12}) $.
        Smaller values as sample size increases indicates a better random number generator.
    }
    \label{fig:random_generators_std_dev}
    \end{center}
\end{figure}

As another test, the autocorrelation function was calculated for the sequences of $ 10^6 $ numbers.
This is plotted in Figure~\ref{fig:random_generators_auto_corr}.
The \texttt{ran0} function again performs the worst: its autocorrelation is higher on average, and we can see peaks in the autocorrelation which indicate that certain elements of the sequence are strongly correlated to each other.
It is difficult to see the plots of the other three functions because of the sheer density of points.
It is important to plot with this density, though, because otherwise important features like the peaks in the \texttt{ran0} plot may be lost.
The other functions perform about the same in terms of autocorrelation: better than \texttt{ran0}, with lower autocorrelations on average and no major peaks.
All of the functions see an increase in the autocorrelation for higher values of $ k $.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Random_generators_auto_corr.png}
    \caption{%
        Absolute value of the autocorrelation function for each random sequence.
        A truly random sequence should have an autocorrelation of close to zero for every $ k $.
    }
    \label{fig:random_generators_auto_corr}
    \end{center}
\end{figure}

\subsection{White noise from random numbers}
\label{subsec:white_noise_from_random_numbers}

The distribution for a uniform random variable $ X $ is given by
\begin{align}
    f_X(x) &= \begin{cases} \frac{1}{b-a} & \text{for } a \leq x \leq b \\ 0 & \text{otherwise} \end{cases}
\end{align}
It is easy to show that this distribution has expected value $ \left\langle X \right\rangle = \frac{1}{2} (a+b) $ and variance $ (\Delta X)^2 = \frac{1}{12} (b-a)^2 $.
We can obtain zero mean and unit variance by setting
\begin{align}
    a = -\sqrt{3} ; \qquad b = \sqrt{3}
\end{align}
In terms of the random variable $ U $ which is uniformly distributed on $ [0, 1) $, $ X $ is given by:
\begin{align}
    X &= 2 \sqrt{3} U - \sqrt{3}
\end{align}

An example of pseudo-randomly generated white-noise is plotted in Figure~\ref{fig:white_noise}, along with its autocorrelation.
As expected, the white noise is centred around zero, and most of the values are between $ -1 $ and $ 1 $.
Also, as expected, the autocorrelation is quite small: between $ 10^{-3} $ and $ 10^{-1} $.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/White_noise.pdf}
    \caption{%
        Sequence of 100 uniform pseudorandom numbers simulating white noise, along with their autocorrelation.
        These were generated with the gfortran random number routine.
    }
    \label{fig:white_noise}
    \end{center}
\end{figure}

In the definition of white noise, there is no requirement in terms of what distribution is used, as long as the mean is zero and the variance is unity.
The Gaussian distribution given by
\begin{align}
    f_X(x) &= \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
\end{align}
satisfies these conditions, and thus it can be used to generate white noise.
This kind of Gaussian white noise is seen in Figure~\ref{fig:white_noise_gaussian}.
Again, the noise is centred around zero, most of the values are between $ -1 $ and $ 1 $, and the autocorrelation is small, as expected.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/White_noise_gaussian.pdf}
    \caption{%
        Sequence of 100 Gaussian pseudorandom numbers simulating white noise, along with their autocorrelation.
        These were generated with Dr Ouyed's random Gaussian number routine.
    }
    \label{fig:white_noise_gaussian}
    \end{center}
\end{figure}

\section{PDFs and Monte Carlo}
\label{sec:pdfs_and_monte_carlo}

A probability distribution function (PDF) must integrate to 1, so the distribution
\begin{align}
    f(x) &= cx \quad \text{on } [0, 1)
\end{align}
must have
\begin{align}
    1 &= \int_{0}^{1} cx \, \d x = \frac{c}{2} \quad \Longrightarrow \quad c = 2
\end{align}

The expected value of a random variable $ X $ with PDF $ f_X(x) $ is
\begin{align}
    E[X] &= \int x f_X(x) \, \d x
\end{align}
and the variance is
\begin{align}
    \text{Var}[X] &= E[X^2] - (E[X])^2
\end{align}

For example, the expected value of a random variable $ X $ with distribution $ f_X(x) = \lambda e^{-\lambda x} $ is
\begin{align}
    E[X] &= \int_{0}^{\infty} x \lambda e^{-\lambda x} \, \d x = \frac{1}{\lambda}
\end{align}
We also have
\begin{align}
    E[X^2] &= \int_{0}^{\infty} x^2 \lambda e^{-\lambda x} \, \d x = \frac{2}{\lambda^2}
\end{align}
which means that
\begin{align}
    \text{Var}[X] &= \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
\end{align}

As an example of a cumulative distribution function (CDF), consider the following table describing the fraction of maintenance checks completed in time less than or equal to $ t $ minutes:

\bigskip
\begin{center}
    \begin{tabular}{cc}
        \toprule
        $ t $ (min) & CDF, fraction completed \\
        \midrule
        0.0  & 0.0  \\
        5.0  & 0.02 \\
        10.0 & 0.08 \\
        15.0 & 0.21 \\
        20.0 & 0.38 \\
        25.0 & 0.80 \\
        30.0 & 0.90 \\
        \bottomrule
    \end{tabular}
\end{center}
\bigskip

We can see that 21\% of checks are completed in 15 minutes or less, 90\% are completed within 30 minutes or less, and 13\% are completed in between 10 and 15 minutes.

\subsection{Beta distribution}
\label{subsec:beta_distribution}

Another example of a PDF is the beta distribution.
With $ \alpha = 3 $ and $ \beta = 2 $, the beta distribution is given by
\begin{align}
    f_X(x) &= \frac{1}{B(3,2)} x^{2} (1-x)
\end{align}
Normalizing:
\begin{align}
    1 &= \int_{0}^{1} f_X(x) \, \d x = \frac{1}{12 B(3,2)} \quad \Longrightarrow B(3,2) = \frac{1}{12}
\end{align}

The CDF is given by
\begin{align}
    F_X(x) &= \int_{0}^{x} 12 x'^2 (1-x') \, \d x' = \left(4 - 3 x \right) x^3
\end{align}

Using the fundamental principle for sampling, we would need to generate a unit, uniform random number $ U $ and solve the equation
\begin{align}
    \left( 4 - 3X \right) X^3  - U &= 0
\end{align}
for $ X $ in the domain $ [0, 1) $.

Using the Newton-Raphson method to solve this equation, $ 10^5 $ samples were generated.
The results are shown in Figure~\ref{fig:beta_fundamental_princ}.
It can be seen from the scatter plot that the distribution is roughly what would be expected of a beta distribution.
The sample mean of this sequence was $ 0.6017 $, the standard deviation was $ 0.03998 $, and the runtime was $ 97.9 $ ms.
The mean and variance are close to the theoretical values of $ 0.6 $ and $ 0.04 $, respectively.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Beta_fundamental_princ.png}
    \caption{%
        Sequence of $ 10^5 $ Beta pseudorandom numbers generated with the fundamental principle.
    }
    \label{fig:beta_fundamental_princ}
    \end{center}
\end{figure}

Another way to generate a Beta sequence is using the accept/reject method.
The maximum value of $ f_X(x) $ is at
\begin{align}
    \frac{\d}{\d x} f_X(x) &= 0 
    \\
    12\left(2x - 3x^2\right) &= 0
    \\
    \left( x - \frac{2}{3} \right) x &= 0
\end{align}
So the maximum value is at $ x = 2/3 $, and it is
\begin{align}
    f_{X,\text{max}} &= 12 \left( 2/3 \right)^2 \left( 1 - 2/3 \right) = \frac{16}{9} \approx 1.7778
\end{align}
To be safe, we use the region $ 0 \leq x < 1 $ and $ 0 \leq y < 1.8 $ for the accept/reject method.

Using this region, $ 10^5 $ samples were generated, and the results are plotted in Figure~\ref{fig:beta_accept_reject}.
It can be seen from the scatter plot that the distribution is roughly what would be expected of a beta distribution.
The sample mean of this sequence was $ 0.6004 $, the standard deviation was $ 0.04001 $, and the runtime was $ 82.3 $ ms.
The mean and variance are even closer to the theoretical values than the values from the fundamental principle sequence, despite having a slightly shorter runtime.
This indicates the effectiveness of the accept/reject method over the fundamental principle method for distributions whose CDF's are not easily invertible.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Beta_accept_reject.png}
    \caption{%
        Sequence of $ 10^5 $ Beta pseudorandom numbers generated with the accept/reject method.
    }
    \label{fig:beta_accept_reject}
    \end{center}
\end{figure}

\subsection{Gaussian distribution}
\label{subsec:gaussian_distribution}

As another example of a non-trivial distribution, we use the Gaussian distribution
\begin{align}
    f_X(x) &= \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[ - \frac{(x - \mu)^2}{2 \sigma^2} \right]
\end{align}
with $ \mu = 5 $ and $ \sigma = 1.25 $.

The peak value of this distribution is
\begin{align}
    f_{X,\text{max}} &= \frac{1}{\sigma \sqrt{2 \pi}} \approx 0.319
\end{align}
So, assuming that $ f_X \approx 0 $ for $ |x - \mu| > 4 \sigma $, we take our accept/reject region to be $ 0 < x < 10 $ and $ 0 < y < 0.35 $.

A generic function, \texttt{gauss\_random}, was created which produces a Gaussian-distributed random variable with desired mean and standard deviation. 
It can be deen in Section~\ref{subsec:random_numbers_module_code}.
The results of $ 10^6 $ trials with a mean of $ 5 $ and a standard deviation of $ 1.25 $ are shown in Figure~\ref{fig:gauss_accept_reject}.
The normalized histogram lines up almost perfectly with the expected distribution, demonstrating that the accept/reject method is an effective way to produce Gaussian-distributed random numbers.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Gauss_accept_reject.pdf}
    \caption{%
        Histogram of a sequence of $ 10^6 $ Gaussian-distributed pseudorandom numbers generated with the accept/reject method.
        Histogram has been normalized so that it corresponds to a PDF.
    }
    \label{fig:gauss_accept_reject}
    \end{center}
\end{figure}

Another way to produce Gaussian-distributed random numbers is by making use of the central limit theorem.
The central limit theorem says that if $ X_i $ are independent and identically-distributed random variables with mean $ \mu $ and standard deviation $ \sigma $, then
\begin{align}
    S_N = \sum_{i=1}^N X_i
\end{align}
approaches a Gaussian-distributed random variable with mean $ N \mu $ and standard deviation $ \sqrt{N} \sigma $ in the limit of large $ N $.

In particular, if $ X_i $ are uniformly distributed on $ [a,b) $, we can solve for $ a $, $ b $, and $ N $ so that $ S_N $ has any desired mean and standard deviation.
If $ \mu_\text{des} $ is the desired mean and $ \sigma_\text{des} $ is the desired standard deviation, then
\begin{align}
    a &= \frac{\mu_\text{des}}{N} - \sqrt{\frac{3\sigma^2_\text{des}}{N}}
    \\
    b &= \frac{\mu_\text{des}}{N} + \sqrt{\frac{3\sigma^2_\text{des}}{N}}
\end{align}

So to generate a Gaussian random number of mean $ \mu_\text{des} $ and standard deviation $ \sigma_\text{des} $, we pick an $ N $: large enough that the central limit theorem applies, but not so large as to waste computation time. 
Then we generate $ N $ uniform random variables on $ [a, b) $, with $ a $ and $ b $ given above.
We add these numbers together to get the desired result.

This method was applied using $ N = 10 $, $ \mu_\text{des} = 5 $ and $ \sigma_\text{des} = 1.25 $.
A sequence of $ 10^4 $ random numbers was created, and the results are plotted in Figure~\ref{fig:gauss_central_limit}.
This histogram clearly lines up with the desired distribution, though the fit is not as clean as it was with the accept/reject method.
Of course, that comparison is unfair since this histogram was generated from a much smaller sample size.

Though this method is an interesting demonstration of the central limit theorem, it was found that it is not an efficient way to generate Gaussian random numbers compared with the accept/reject approach.
This is not surprising since at least 10 uniform random numbers must be generated for each Gaussian random number produced.
For more accurate distribution fits, even more uniform random numbers must be used in the sum.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Gauss_central_limit.pdf}
    \caption{%
        Histogram of a sequence of $ 10^4 $ Gaussian-distributed pseudorandom numbers generated with the central limit theorem method.
        Histogram has been normalized so that it corresponds to a PDF.
    }
    \label{fig:gauss_central_limit}
    \end{center}
\end{figure}

\section{Markov chain}
\label{sec:markov_chain}

\subsection{Weather example}
\label{subsec:weather_example}

Consider a classic Markov chain: the weather in a city is only sunny or cloudy, and the weather tomorrow only depends on the weather today.
Let $ X_i $ be the weather on day $ i $, where $ X_i = 1 $ corresponds to sunny and $ X_i = 0 $ corresponds to cloudy.
Then the relevant probabilities are
\begin{align}
    P[1|1] = P[X_{i+1} = 1 | X_{i} = 1] &= 0.75
    \\
    P[0|1] = P[X_{i+1} = 0 | X_{i} = 1] &= 0.25
    \\
    P[1|0] = P[X_{i+1} = 1 | X_{i} = 0] &= 0.50
    \\
    P[0|0] = P[X_{i+1} = 0 | X_{i} = 0] &= 0.50
\end{align}

The transition equation in matrix form is
\begin{align}
    \mat{P[X_{i+1} = 1] \\ P[X_{i+1} = 0]} &= \mat{P[1|1] & P[1|0] \\ P[0|1] & P[0|0]} \mat{P[X_{i} = 1] \\ P[X_{i} = 0]}
    \\
    \mat{P[X_{i+1} = 1] \\ P[X_{i+1} = 0]} &= \mat{0.75 & 0.50 \\ 0.25 & 0.50} \mat{P[X_{i} = 1] \\ P[X_{i} = 0]}
\end{align}

In the limiting case, we have
\begin{align}
    \mat{P[X_\infty = 1] \\ P[X_\infty = 0]} &= \mat{0.75 & 0.50 \\ 0.25 & 0.50} \mat{P[X_\infty = 1] \\ P[X_\infty = 0]}
    \\
    \Longrightarrow \quad \mat{0 \\ 0} &= \mat{-0.25 & 0.50 \\ 0.25 & -0.50} \mat{P[X_\infty = 1] \\ P[X_\infty = 0]}
\end{align}
Which implies that $ P[X_\infty = 1] = 2 P[X_\infty = 0] $.
Insisting that the probabilities add up to 1 gives the steady-state probabilities.
\begin{align}
    \mat{P[X_\infty = 1] \\ P[X_\infty = 0]} &= \mat{2/3 \\ 1/3}
\end{align}

Alternatively, we can find that the eigenvectors of the transition matrix are $ [2,1]^T $ and $ [-1,1]^T $ with eigenvalues $ 1 $ and $ 1/4 $, respectively.
We want the eigenvector corresponding to the eigenvalue $ 1 $: the matrix applied to the steady state probabilities should return the same state.
So we discard the eigenvector corresponding to the eigenvalue $ 1/4 $.
(This is not surprising, since that eigenvector would give negative probabilities).
Normalizing so that the probabilities add up to 1, we get the same result:
\begin{align}
    \mat{P[X_\infty = 1] \\ P[X_\infty = 0]} &= \mat{2/3 \\ 1/3}
\end{align}

A simulation of this Markov Chain was performed, with the results shown in Figure~\ref{fig:weather_simulation}.
In calculating the probability of a sunny day, the first 10 days were discarded (``burn-in'').
It is seen that the calculated probabilities do not exactly match the theoretical ones.
However, with an increased number of trials, the values do converge to the theoretical ones.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Weather_simulation.pdf}
    \caption{%
        Simulation of the weather Markov Chain.
        The calculated frequencies are plotted with black boxes, and the expected probabilities are shown with vertical red lines.
        On the $ x $-axis, state 0 is rainy day, and state 1 is sunny day (as in the analysis above).
    }
    \label{fig:weather_simulation}
    \end{center}
\end{figure}

\subsection{Edged square}
\label{subsec:edged_square}

Let $ P_{n \to m} $ be the probability of moving from corner $ n $ to corner $ m $, and let $ P_n[i] $ be the probability of being on corner $ n $ on the $ i $th move.
Then the transition equation is
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1] \\ P_4[i+1]} &= 
    \mat{P_{1 \to 1} & P_{2 \to 1} & P_{3 \to 1} & P_{4 \to 1} \\
         P_{1 \to 2} & P_{2 \to 2} & P_{3 \to 2} & P_{4 \to 2} \\
         P_{1 \to 3} & P_{2 \to 3} & P_{3 \to 3} & P_{4 \to 3} \\
         P_{1 \to 4} & P_{2 \to 4} & P_{3 \to 4} & P_{4 \to 4} }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i] \\ P_4[i]} 
\end{align}
Plugging in the values, we get
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1] \\ P_4[i+1]} &= 
    \mat{0 & 1/2 & 1/3 & 1/2 \\
         1/3 & 0 & 1/3 & 0 \\
         1/3 & 1/2 & 0 & 1/2 \\
         1/3 & 0 & 1/3 & 0 }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i] \\ P_4[i]} 
\end{align}

The steady state condition is then
\begin{align}
    \mat{-1 & 1/2 & 1/3 & 1/2 \\
         1/3 & -1 & 1/3 & 0 \\
         1/3 & 1/2 & -1 & 1/2 \\
        1/3 & 0 & 1/3 & -1 }
    \mat{P_1[\infty] \\ P_2[\infty] \\ P_3[\infty] \\ P_4[\infty]} &= \mat{0 \\ 0 \\ 0 \\ 0 }
\end{align}
which has solutions
\begin{align}
    P_1[\infty] &= \frac{3}{2} P_4[\infty]
    \\
    P_2[\infty] &= P_4[\infty]
    \\
    P_3[\infty] &= \frac{3}{2} P_4[\infty]
\end{align}

With the condition that all the probabilities add up to unity, we get
\begin{align}
    P_1[\infty] &= \frac{3}{10}
    \\
    P_2[\infty] &= \frac{1}{5}
    \\
    P_3[\infty] &= \frac{3}{10}
    \\
    P_4[\infty] &= \frac{1}{5}
\end{align}

Alternatively, we can show that the eigenvectors are $ [1.5, 1, 1.5, 1]^T $, $ [-1, 1, -1, 1]^T $, $ [-1, 0, 1, 0]^T $, $ [0, -1, 0, 1]^T $ with eigenvalues $ 1 $, $ -2/3 $, $ -1/3 $, and $ 0 $, respectively.
Normalizing the eigenvector corresponding to eigenvalue 1 so that its elements add up to unity gives the result above.

This system was simulated four times in Monte Carlo, once with each possible starting state (corner).
For each run, a burn-in period of 50 iterations was used, followed by 500 iterations which were used to estimate the probabilities.
The results are plotted in Figure~\ref{fig:square_simulation}.
It can be seen that the results are in good alignment with the probabilities expected from theory.
Increasing the number of iterations would improve the match.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Square_simulation.pdf}
    \caption{%
        Simulation of the edged square Markov Chain.
        The calculated frequencies are plotted with black boxes, and the expected probabilities are shown with vertical red lines.
        On the $ x $-axis, state $ n $ corresponds to corner $ n $.
    }
    \label{fig:square_simulation}
    \end{center}
\end{figure}


\subsection{Business example}
\label{subsec:business_example}

As another example, we look at customer transitions between brands.
The transition equation is given by
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1]} &= 
    \mat{P_{1 \to 1} & P_{2 \to 1} & P_{3 \to 1} \\
         P_{1 \to 2} & P_{2 \to 2} & P_{3 \to 2} \\
         P_{1 \to 3} & P_{2 \to 3} & P_{3 \to 3} }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i] \\ P_4[i]} 
\end{align}
\begin{align}
    \mat{P_1[i+1] \\ P_2[i+1] \\ P_3[i+1]} &= 
    \mat{0.80 & 0.03 & 0.20 \\
         0.10 & 0.95 & 0.05 \\
         0.10 & 0.02 & 0.75 }
    \mat{P_1[i] \\ P_2[i] \\ P_3[i]} 
\end{align}

The initial state (month 1) is.
\begin{align}
    \mat{P_1[1] \\ P_2[1] \\ P_3[1]} &= \mat{0.45 \\ 0.25 \\ 0.30}
\end{align}

We find the month 3 state by applying the transition matrix twice:
\begin{align}
    \mat{P_1[3] \\ P_2[3] \\ P_3[3]} &= 
    \mat{0.80 & 0.03 & 0.20 \\
         0.10 & 0.95 & 0.05 \\
         0.10 & 0.02 & 0.75 }^2
    \mat{0.45 \\ 0.25 \\ 0.30} 
    \\
    \mat{P_1[3] \\ P_2[3] \\ P_3[3]} &= \mat{0.405925 \\ 0.339125 \\ 0.254950}
\end{align}

The steady state solution is given by the eigenvector corresponding to an eigenvalue of 1.
After normalizing this eigenvector so that the elements add up to unity, we get the steady state solution as
\begin{align}
    \mat{P_1[\infty] \\ P_2[\infty] \\ P_3[\infty]} &= \mat{0.237113 \\ 0.618556 \\ 0.144330}
\end{align}
So this model predicts that Brand 2 will take most of the market share in the long run.

Whether the market will actually approach these values depends strongly on our confidence in the model.
We have assumed that the transition rates do not change from month to month, which is probably not a valid assumption long-term.
This ignores a wide array of unexpected events which could effect market shares.

\section{Random-walk Metropolis}
\label{sec:random_walk_metropolis}

In this section, we use the random walk metropolis algorithm as a way to sample from arbitrary (possibly complicated) distributions.
First, a code was written to perform a pure random walk.
At each step, the point moves a distance $ \delta = 1 $ either to the right or to the left with equal probability.
After 200 steps, the position is recorded.
Repeating this trial $ 10^4 $ times, we can use a histogram to estimate the probability density.

The resulting distribution is binomial, but because the number of steps is large (200), the central limit theorem suggests that it is a good approximation to a Gaussian distribution.
This is confirmed by the simulation results, plotted in Figure~\ref{fig:simple_random_walk}.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Simple_random_walk.pdf}
    \caption{%
        Histogram of simple random walk simulation.
    }
    \label{fig:simple_random_walk}
    \end{center}
\end{figure}

A more interesting test uses the random walk Metropolis algorithm to sample a Gaussian distribution proportional to $ \exp(-0.2x^2) $.
The distribution was calculated using a starting point of 0, 5000 trials, and a uniform proposal distribution of width equal to the standard deviation of the target distribution.
The convergence of the mean is shown in Figure~\ref{fig:metropolis_gauss_mean}.
The mean gets ``roughly'' close within a few thousand iteration, but the convergence rate gets slower as the number of iterations increases.
It takes tens of thousands of iterations before the mean is negligibly far from the actual value.

The histogram resulting from this random walk is plotted in Figure~\ref{fig:metropolis_gauss_distribution}.
The histogram is a fairly good fit compared to the exact one, and the fit gets closer as the number of trials increases.
More importantly, this algorithm is efficient: it took 6 ms to run while the above method took 33 ms.
(That's because in the above algorithm, 200 uniform random numbers have to be generated for each sample, while in this method only 1 uniform random number needs to be generated per sample).

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Metropolis_Gauss_mean.pdf}
    \caption{%
        Convergence of the mean for a Metropolis sampling of a Gaussian distribution.
        The blue line is the actual mean.
    }
    \label{fig:metropolis_gauss_mean}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Metropolis_Gauss_distribution.pdf}
    \caption{%
        Gaussian distribution estimated with a Metropolis random walk. 
        The blue line is the actual distribution.
    }
    \label{fig:metropolis_gauss_distribution}
    \end{center}
\end{figure}

As another test, the Metropolis-Hastings algorithm was used in three different situations, seen in Figure~\ref{fig:mh_hist}.
In each case $ 10^6 $ trials were performed.

In the first case, the target distribution was $ N(5,1) $ and the proposal distribution was $ N(-5,1) $.
It is seen that this choice of target distribution was extremely poor.
Most of the time, the point did not move at all, and if it did, it tended to move to the left.
Thus, it did a very poor job exploring the space.

In the second case, the target distribution was $ N(0,5) $ and the proposal distribution was $ N(0,0.5) $.
This is a much better choice of proposal distribution, giving a close match to the actual target distribution.

In the third case, the target distribution was $ t(1) $ and the proposal distribution was $ N(0,1) $.
Again, this proposal distribution and the Metropolis-Hastings algorithm did a good job exploring the target distribution.
This particular target distribution is much wider than the Gaussian in terms of tails (despite having the same mean), making it more challenging to effectively sample.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Student_t.pdf}
    \caption{%
        Student's t distribution for different parameter values.
        For high $ \nu $, the distribution lines up with a Gaussian distribution.
    }
    \label{fig:student_t}
    \end{center}
\end{figure}


\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/MH_hist.pdf}
    \caption{%
        Distributions estimated with the Metropolis-Hastings algorithm.
        Actual distributions are plotted in blue.
    }
    \label{fig:mh_hist}
    \end{center}
\end{figure}

As a final example, we use Metropolis-Hastings to sample the following PDF:
\begin{align}
    P(x) &= C x^{-5/2} e^{-2/x}
\end{align}

For a symmetric proposal distribution, the acceptance probability of moving from $ x = 1 $ to $ x = 39.82 $ is
\begin{align}
    A(1 \to 39.82) &= 0.00001422
\end{align}
This transition is extremely unlikely to happen.

This target distribution was sampled using a proposal distribution $ N(0, 100) $ with a starting point of 1 and 500 iterations.
See Figure~\ref{fig:mh_mixing_normal}.
This is an example of a poorly-mixed sequence.
Because of the large standard deviation of the proposal distribution, only a very small percentage of the point are accepted, leading to poor exploration of the space.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/MH_mixing_normal.pdf}
    \caption{%
        Complicated distribution sampled using Metropolis-Hastings with a $ N(0, 100) $ proposal distribution.
    }
    \label{fig:mh_mixing_normal}
    \end{center}
\end{figure}

The same target distribution was sampled again using a $ \chi^2_1 $ distribution, which is highly asymmetric.
See Figure~\ref{fig:mh_mixing_chi2}.
This is another example of a poorly-mixed sequence.
In this case, the acceptance rate is good, but because the distribution is so asymmetric, this means that most of the accepted movements are in one direction.
Thus the walk tends to continue moving up, doing a poor job exploring the space.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/MH_mixing_chi2.pdf}
    \caption{%
        Complicated distribution sampled using Metropolis-Hastings with a $ \chi^2_1 $ proposal distribution.
    }
    \label{fig:mh_mixing_chi2}
    \end{center}
\end{figure}

Both of these examples demonstrate the importance of selecting a good proposal distribution in the Metropolis-Hastings algorithm.
Choosing a poor proposal distribution gives a poorly-mixed chain which does a poor job exploring the space.
The result is then a bad estimate of the target distribution.
We should aim to pick a proposal distribution so that the walk explores the space thoroughly and efficiently.

\section{Simulated Annealing}
Simulated Annealing is a useful tool to search for the global minimum or maximum. We will show this using the following equation.
\begin{equation}
V(x) = x^4 - x^2 + 0.1x
\end{equation}
we can see in Figure \ref{fig:SA1} that there is not only a global minimum, but also a local minimum at $\pm 5$. Figure \ref{fig:SA1} shows the initial temperature of 1.0. We can see a wide exploration of the space, spanning both sides of the equation. When we lower the temperature to 0.4, as in figure \ref{fig:SA04} we see a bit more of focus around the global minimum. Again decreasing the temperature to 0.1, as in figure \ref{fig:SA01}, we see an even more narrow focus on the solution. This behaviour is a property of simulated annealing. With a higher temperature we see more exploration of the system, and as the temperature decreases, the simulation begins to converge on the solution. The entire simulation is displayed in figure \ref{fig:SAAll}. We see that most of the time was spent on the two solutions, and did find the global minimum of -0.26 at the point -0.54. Comparing this to the analytical solution of -0.32 at the point -0.73, the found solution is close. Simulated annealing is not a method to always find the global, but rather a close enough solution. The advantage to this method is for very complex, and multidimensional functions, Simulated Annealing is more efficient than directly solving for the maximum or minimum.  

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/SA1.pdf}
    \caption{%
        Left: The time series exploration of the function for a temperature of 1.0
        Right: Histogram showing the distribution of the the points explored for a temperature of 1.0
    }
    \label{fig:SA1}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/SA04.pdf}
    \caption{%
        Left: The time series exploration of the function for a temperature of 0.4
        Right: Histogram showing the distribution of the the points explored for a temperature of 0.4
    }
    \label{fig:SA04}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/SA01.pdf}
    \caption{%
        Left: The time series exploration of the function for a temperature of 0.1
        Right: Histogram showing the distribution of the the points explored for a temperature of 0.1
    }
    \label{fig:SA01}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/SA_All.pdf}
    \caption{%
        Left: The time series exploration of the function for the entire simulation
        Right: Histogram showing the distribution of the the points explored for the entire simulation
            }
    \label{fig:SAAll}
    \end{center}
\end{figure}

\section{Birthday Problem}
Using the prescribed method, we were able to simulate 30 people having random birthdays 10000 times. We found that approximately 7100 trials out of 1000 had one or more people sharing birthdays. 

\section{Finance}
 We were able to simulate a stock using the following formula. 
 \begin{equation}
 dS = rS(t)dt + vS(t)\epsilon \sqrtsign{dt}
 \end{equation}
 We can make this a discrete process by transforming from infinitesimal to discrete changes in dS and dt and arrive at the following.
 \begin{equation}
 S_{n+1} = S_{n} + rS_{n}\Delta t + vS_{n}\epsilon \sqrtsign{\Delta t}
 \end{equation}
 Where $\epsilon$ is a normally distributed random variable. Figure \ref{fig:Finance} shows two values for $\Delta t$, one being 3 days and 8 hours, and the effects they have on the stock, both over a period of one year. We can see a more granular movement in the stock prices, which in turn can lead to a more accurate simulation. Running this simulation yields similar end results, however the path to that point does vary significantly. This is the random nature playing it's effect. 
 
 \begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Finance1.pdf}
    \caption{%
        Left: Simulation of stock prices over a one year period with a 3 day interval
        Right: Stock prices simulation over one year with an 8 hour interval
            }
    \label{fig:Finance}
    \end{center}
\end{figure}
 
 If we instead start looking at the logarithm of the change of stock prices, and substitute $L=log(S)$, $\mu = rdt$, and $\sigma = v\sqrtsign{dt}$ we can derive the equation:
 \begin{equation}
 dL = \mu + \sigma\epsilon
 \end{equation}
 where $\epsilon$ is a normally distributed random variable. 
 Thus $ dL $ is a normally distributed random variable with a variance of $\sigma^2$ and mean of $\mu$. To find the balance point between the two, we can set the two equal to each other and find that the are the same for a time interval $\Delta t = \frac{v^{2}}{r^{2}}$.
Here we have naively replaced the infinitesimal $ dt $ with the finite $ \Delta t $.
This can actually be justified with the central limit theorem

The actual formula for $\Delta L$ is essentially an integral of infinitesimal random variables $dL_{i}$:
\begin{equation}
    \Delta L = \lim_{dt \to 0} \sum^{N}_{i=1} dL_i
\end{equation}
 
where
\begin{align}
    N=\frac{\Delta t}{dt} 
\end{align}
The Central Limit Theorem says the for a large $N$ (which it will be as $ dt \to 0 $), $\Delta L$ is a gaussian distribution with expected value
\begin{align}
    \text{E}[\Delta L]=N \cdot \text{E}[dL_{i}]=\frac{\Delta t}{dt}rdt = r dt 
\end{align}
and variance
\begin{align}
    \text{Var}[\Delta L]=N \cdot \text{Var}[dL_{i}] = \frac{\Delta t}{dt}v^{2}dt = \Delta tv^{2} 
\end{align}
So, to to find the distribution of $ \Delta L $, we simply take the distribution of $ dL $ and replace $ dt $ with $ \Delta t $.
This is a somewhat surprising result, confirming our naive approach.

So with $r=0.12$ and $v=0.2$ we can expect an change in price of 1.02 with a variance of 0.003. 
With such a small variance we can see that there is a likely to be a small amount of money made in a time period of one month. 
It would however be better to wait longer for larger expectation values, however this comes at the cost of an increase variance, increasing the probability of not having the expected return.

\section{Metropolis-Hastings and the Ising model}
\label{sec:metropolis_hastings_and_the_ising_model}

As an application of the Metropolis-Hastings Monte Carlo method, we simulate a system of $ N $ particles following the Ising model.
The $ i $th particle has spin $ s_i $ which is either up ($ +1 $) or down ($ -1 $), and the energy of the system is given by
\begin{align}
    E &= -J \sum_{\substack{i,j \\ \text{pairs}}} s_i s_j - \mu H \sum_{i=1}^{N} s_i
\end{align}
With some simplifying assumptions, a simulation was made of a $ 32 \times 32 $ system of particles.
It was run with temperatures $ T = 1, 2.25, 2.5, $ and $ 3 $.
For each run, $ N_\text{MC} = 12\,000 $ sweeps were performed to ensure that the means of the quantities of interest settled.

The energy of the system plotted in Figure~\ref{fig:ising_energy}.
It is seen in this plot that by around 100 sweeps (conservatively), the energy appears to have settled into its equilibrium variation for all temperatures.
The number of sweeps taken appears to depend on temperature, with the $ T = 2.25 $ curve taking longer to settle in than the others, and the $ T = 1.00 $ curve settling quite quickly.

These energy plots behave exactly as expected.
The initial state, with all spins up, is the ground state of the system.
For the low temperature $ T = 1.00 $, the system spends much of its time in the ground state and only occasionally jumps into higher energy states.
With higher temperatures, the higher energy states become more and more probable, and we see the energy curve move upward.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Ising_energy.pdf}
    \caption{%
        Energy from the Ising model simulation for each temperature as a function of sweep number.
        Only the first 1000 sweeps are included in the plot so that the relevant detail is not lost.
    }
    \label{fig:ising_energy}
    \end{center}
\end{figure}

The mean energies are plotted in Figure~\ref{fig:ising_mean_energy}.
After around 10000 iterations, the means of all the energies have stopped fluctuating noticeably.
(More importantly, the mean magnetizations have settled at this point.
Magnetization is discussed later.)
This is most easily seen by plotting the results, however a more rigorous condition would be to require that the largest change in the last 10 cycles is less than some threshold.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Ising_mean_energy.pdf}
    \caption{%
        Mean energy of the system as a function of cycles (discarding the first 500).
    }
    \label{fig:ising_mean_energy}
    \end{center}
\end{figure}

It should be noted that the number of accepted information increases as a function of temperature $ T $.
This is because as $ T $ increases, the probability of the system moving into higher energy states increases.

The magnetization was also calculated for each sweep.
These results are plotted in Figures~\ref{fig:ising_magnetization} and \ref{fig:ising_mean_magnetization_vs_temp}.
First, note that the variation of the average magnetization as a function of temperature matches up nicely with the predicted theoretical results.
Second, note that negative magnetizations have the same energy as positive magnetizations.
Thus, it is possible for the system to ``flip'' its orientation, though it is extremely unlikely at low temperatures.
This flip would be most likely at just below the Curie temperature, where the magnetization has a noticeable magnetude, but the temperature makes a change in state more likely.

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Ising_magnetization.pdf}
    \caption{%
        Mean magnetization of the Ising model as a function of sweep number.
        Only the first 2000 sweeps are shown.
    }
    \label{fig:ising_magnetization}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{../Plots/Ising_mean_magnetization_vs_temp.pdf}
    \caption{%
        Mean energy of the system as a function of cycles (discarding the first 500).
    }
    \label{fig:ising_mean_magnetization_vs_temp}
    \end{center}
\end{figure}



\section{Traveling Salesman}
We were able to build the 3 solutions to the traveling salesman problem. 
The attached animated gif displays our solutions. 
We found that both the brute force and the nearest neighbour were able to find the shortest round trip of 16. 
Using Simulated Annealing found a solution of 18 Units. 
Where Simulated Annealing shines is in the time complexity of it's solutions. 
The brute force method has a time complexity of $O((n-1)!)$ since we know the starting points. 
This solution requires n-1 nested loops, and we found a run time of approximately 5 seconds. 
The shortest distance algorithm has a time complexity of $O(n^{2})$. 
While this is a nicer solution, we still found a run time of 5 seconds. 
The time complexity of the simulated annealing approach is different, as it does not scale with the number of points. 
Instead it scales with the temperature range and the cooling schedule. 
It's time complexity is $O(\Delta T \times (T_{increment} + \sigma))$, where $\Delta T$ is the temperature range, $T_{increment}$ is the amount the temperature is cooled each time, and $\sigma$ is the number of iterations before cooling. 
The important thing to note here is none of these directly scale with the complexity of the problem. 
It is wise to adjust these schedules for larger problems, however that is at your discretion. 
We were able to find run that section of code in approximately 2 seconds.

\section{Conclusion}
\label{sec:conclusion}

In this report we have explored in detail a number of applications of Monte Carlo methods.
The wide range of applicability of this method is astonishing.

(Also this assignment took a really, really long time!)

\onecolumn

\section{Code}
\label{sec:code}

\subsection{Random generators code}
\label{subsec:random_generators_code}

\lstinputlisting[breaklines]{../Random_generators.f90}
\vspace{10pt}

\subsection{Random numbers module code}
\label{subsec:random_numbers_module_code}

\lstinputlisting[breaklines]{../../Modules/Random_numbers_module.f90}
\vspace{10pt}

\subsection{Random generators plotting code}
\label{subsec:random_generators_plotting_code}

\lstinputlisting[breaklines]{../Random_generators.gp}
\vspace{10pt}

\subsection{PDF and Monte Carlo code}
\label{subsec:pdf_and_monte_carlo_code}

\lstinputlisting[breaklines]{../PDF_and_Monte_Carlo.f90}
\vspace{10pt}

\subsection{PDF and Monte Carlo plotting code}
\label{subsec:pdf_and_monte_carlo_plotting_code}

\lstinputlisting[breaklines]{../PDF_and_Monte_Carlo.gp}
\vspace{10pt}

\subsection{Markov chain Monte Carlo code}
\label{subsec:markov_chain_monte_carlo_code}

\lstinputlisting[breaklines]{../MCMC.f90}
\vspace{10pt}

\subsection{Markov chain Monte Carlo plotting code}
\label{subsec:markov_chain_monte_carlo_plotting_code}

\lstinputlisting[breaklines]{../MCMC.gp}
\vspace{10pt}

\subsection{Metropolis-Hastings code}
\label{subsec:metropolis_hastings_code}

\lstinputlisting[breaklines]{../Metropolis_Hastings.f90}
\vspace{10pt}

\subsection{Metropolis-Hastings module}
\label{subsec:metropolis_hastings_module}

\lstinputlisting[breaklines]{../../Modules/Metropolis_Hastings_module.f90}
\vspace{10pt}

\subsection{Metropolis-Hastings plotting code}
\label{subsec:metropolis_hastings_plotting_code}

\lstinputlisting[breaklines]{../Metropolis_Hastings.gp}
\vspace{10pt}

\subsection{Simulated annealing code}
\label{subsec:simulated_anealing_code}

\lstinputlisting[breaklines]{../Simulated_Anealing.f90}
\vspace{10pt}

\subsection{Simulated annealing plotting code}
\label{subsec:simulated_anealing_plotting_code}

\lstinputlisting[breaklines]{../SA.gp}
\vspace{10pt}

\subsection{Birthday problem code}
\label{subsec:birthday_problem_code}

\lstinputlisting[breaklines]{../birthday.f90}
\vspace{10pt}

\subsection{Finance code}
\label{subsec:finance_code}

\lstinputlisting[breaklines]{../Finance.f90}
\vspace{10pt}

\subsection{Finance plotting code}
\label{subsec:finance_plotting_code}

\lstinputlisting[breaklines]{../Finacne1.gp}
\vspace{10pt}

\subsection{Ising code}
\label{subsec:ising_code}

\lstinputlisting[breaklines]{../Ising.f90}
\vspace{10pt}

\subsection{Ising plotting code}
\label{subsec:ising_plotting_code}

\lstinputlisting[breaklines]{../Ising.gp}
\vspace{10pt}

\subsection{Traveling salesman code}
\label{subsec:traveling_salesman_code}

\lstinputlisting[breaklines]{../Traveling_Salsesman.f90}

\subsection{Traveling salesman plotting code}
\label{subsec:traveling_salesman_plotting_code}

\lstinputlisting[breaklines]{../Traveling.gp}


\end{document}
