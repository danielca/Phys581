\documentclass[twocolumn]{myarticle}

\usepackage{mymacros}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{mathtools}

\lstset{%
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
stepnumber=1,}

\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\d}{\mathrm{d}}

\begin{document}

\title{Physics 581, Lab 3:\\Numerical solution of partial differential equations}
\author{Casey Daniel and Chris Deimert}
\date{\today}

\maketitle

\section{Introduction}
\label{sec:introduction}

Partial differential equations appear as the central modelling equations in a huge number of applications.
For interesting applications, these equations are rarely solvable analytically.
As such, we must usually resort to numerical methods in order to solve them.

In this report, we look specifically at finite-difference methods.
We look at a wide range of applications from electromagnetism to fluid dynamics to quantum mechanics to econo-physics.
In each case, we look at the pros and cons of different schemes in terms of speed, accuracy, and stability.

\section{Warm up}
\label{sec:warm_up}

\subsection{The direct method}
\label{subsec:the_direct_method}

The direct method for solving a PDE involves applying a finite difference and then directly solving the resulting matrix equation.
The key question is, can we always turn a finite difference equation into a matrix equation?

Suppose we have a problem of the form
\begin{align}
    \mathcal{L} u(x, y) &= \rho(x, y)
\end{align}
where $ \mathcal{L} $ is a linear operator.
For example, if we took $ \mathcal{L} = \partial_x^2 + \partial_y^2 $ we would have Poisson's equation.

When we discretize this equation, we get an equation of the form
\begin{align}
    \mathcal{L}_d u[n,m] &= \rho[n, m] \label{eq:general_difference_eq}
\end{align}
with
\begin{align}
    n &= 1, 2, \ldots, N
    \\
    m &= 1, 2, \ldots, M
\end{align}

For example, for the Poisson equation operator with grid spacing $ h $, the discretized version would be
\begin{align}
    \mathcal{L}_d &= \frac{\nabla_x \Delta_x + \nabla_y \Delta_y}{h^2}
\end{align}
where $ \nabla $ and $ \Delta $ are the backward and forward difference operators defined in Chapter 12 of the textbook.

Now, $ u[n,m] $ and $ \rho[n,m] $ can be seen as $ N\times M $ matrices, which are elements of an $ N \cdot M $-dimensional vector space.
It is also not hard to see that if $ \mathcal{L} $ is a linear operator, then its discretized version $ \mathcal{L}_d $ is a linear operator on this vector space.

From linear algebra, we know that we can write any element of an $ N \cdot M $ vector space as an $ N\cdot M $ element column vector.
So we can write $ u[n,m] $ and $ \rho[n,m] $ as column vectors $ \vect{u} $ and $ \vect{\rho} $.
We also know that we can write any linear operator on an $ N \cdot M $ dimensional vector space as an $ \left( N \cdot M \right) \times \left( N \cdot M \right) $ matrix.
So we can write $ \mathcal{L}_d $ as a matrix $ \vect{L} $.
Thus, we can write equation~\eqref{eq:general_difference_eq} as a matrix equation!
\begin{align}
    \vect{L} \cdot \vect{u} &= \vect{d}
\end{align}

The direct method then involves solving this matrix equation directly, and there are a number of well-known ways to do this.
In particular, $ \vect{L} $ will usually be a sparse matrix with some sort of banded structure, so we can take advantage of fast algorithms designed for this type of matrix.

The only piece of this which we have not discussed is how to define the maps
\begin{align}
    u[n,m] &\to \vect{u}
    \\
    \rho[n,m] &\to \vect{\rho}
    \\
    \mathcal{L}_d &\to \vect{L}
\end{align}
We know that this can be done from linear algebra theory, and it is not hard in practise.
Basically, we are just re-indexing the matrix $ u[n,m] $ so that it only has one index.
For example:
\begin{align}
    \mat{0 & 1 & 2 \\ 3 & 4 & 5 } &\to \mat{0 \\ 1 \\ 2 \\ 3 \\ 4 \\ 5 }
\end{align}

In summary, for linear differencing schemes, we can take advantage of the linearity to write the entire equation as a matrix equation.
We can then directly solve this matrix equation to get our entire solution in one shot.
The first challenge is translating the difference equation into a matrix equation, though we know that we can do it from linear algebra.
The second challenge is solving the matrix equation, though we can take advantage of properties of the $ \vect{L} $ matrix such as sparseness.

\subsection{Iterative solution method}
\label{subsec:iterative_solution_method}

From the section above, we know that for linear differential equations, we can write a linear difference equation of the form
\begin{align}
    \vect{L} \cdot \vect{u} &= \vect{\rho}
\end{align}

Now, let us split $ \vect{L} $ into two parts:
\begin{align}
    \vect{L} &= \vect{L}_1 - \vect{L}_2
\end{align}
so that
\begin{align}
    \vect{L}_1 \cdot \vect{u} &= \vect{L}_2 \cdot \vect{u} + \vect{\rho}
\end{align}

The iterative solution method starts with a guess solution $ \vect{u}^{(0)} $ and iteratively generates a set of solutions $ u^{(r)} $ using
\begin{align}
    \vect{L}_1 \cdot \vect{u}^{(r)} = \vect{L}_2 \cdot \vect{u}^{(r-1)} + \vect{\rho}
\end{align}
In the limit $ r \to \infty $, we expect to approach a steady-state solution where $ \vect{u}^{(r-1)} \approx \vect{u}^{r} $.
So taking $ r \to \infty $ we get
\begin{align}
    \vect{L}_1 \cdot \vect{u}^{(\infty)} &= \vect{L}_2 \cdot \vect{u}^{(\infty)} + \vect{\rho}
\end{align}
and we can see that $ \vect{u}^{(\infty)} $ corresponds to the solution we are looking for.

This method can be advantageous over the direct method if we pick $ \vect{L}_1 $ and $ \vect{L}_2 $ judiciously.
In particular, we want $ \vect{L}_1 $ to be easily-invertible so that the iteration equation can be solved quickly without computationally-expensive linear system algorithms.

The downside is that we now must solve a large number of linear equations.
If this method is to be useful, it needs to converge reasonably fast.
If the convergence is slow, it may not be the best method for the problem at hand.

\subsection{Jacobi and Gauss-Seidel iteration methods}
\label{subsec:jacobi_and_gauss_seidel_iteration_methods}

The Jacobi and Gauss-Seidel iteration methods again solve equations of the form
\begin{align}
    \mathcal{L} u(x, y) &= \rho(x, y)
\end{align}

The idea here is to add a time dependence:
\begin{align}
    \frac{\partial u(x, y, t)}{\partial t} + \mathcal{L} u(x, y, t) &= \rho(x, y)
\end{align}
If we solve this time-dependent equation for some initial condition (a ``guess'' solution) we can find the solution to the time-independent equation by taking the limit $ t \to \infty $.
This method is known as a relaxation method because the time-dependent solution ``relaxes'' to the time-independent solution.

The Jacobi and Gauss-Seidel methods essentially discretize this time-dependent equation to solve it numerically.
The only difference between the two methods lies in how the discretization is done.

The Jacobi method uses a forward difference for the time derivative and central differences for the spatial derivatives. If we use a spatial grid size of $ h $ and a time step of $ k = h^2/4 $ (which is right at the stability condition) then we obtain the following update equation:
\begin{align}
    u^{n+1}_{j, l} &= \frac{1}{4} \left( u^{n}_{j+1, l} + u^{n}_{j-1, l} + u^{n}_{j, l+1} + u^{n}_{j, l-1} \right) - \frac{h^2}{4} \rho_{j,l}
\end{align}
So we obtain $ u^{n+1}_{j, l} $ by averaging the four adjacent points at the last time step and then adding the source term.
To obtain the time-independent solution $ u_{j, l} $, we simply apply this update equation until the difference between successive terms is below some threshold.

The Gauss-Seidel method is quite similar, but it uses values from the $ n+1 $ time step as soon as they are available.
The update equation is:
\begin{align}
    u^{n+1}_{j, l} &= \frac{1}{4} \left( u^{n}_{j+1, l} + u^{n+1}_{j-1, l} + u^{n}_{j, l+1} + u^{n+1}_{j, l-1} \right) - \frac{h^2}{4} \rho_{j,l}
\end{align}
This method will converge to a solution slightly faster than the Jacobi method, though they are both quite slow.

Though it's not clear from this presentation, both of these methods are examples of the matrix iteration method described in Section~\ref{subsec:iterative_solution_method}.
This presentation is useful, though, because it gives a nice physical analogy: solving a time-independent equation by using the time-dependent equation in the limit $ t \to \infty $.

\subsection{The Laplace equation}
\label{subsec:the_laplace_equation}

As an example of the above methods, we solve the Laplace equation
\begin{align}
    u_{xx} + u_{yy} &= 0
\end{align}
in the region $ x, y \in [0, 1] $ with boundary conditions
\begin{align}
    u(x,0) &= 100 \sin \left( 2 \pi x \right)
    \\
    u(x,1) &= -100 \sin \left( 2 \pi x \right)
    \\
    u(0, y) &= 0
    \\
    u(1, y) &= 0
\end{align}
We can think of this as solving for the voltage in a waveguide, for example.

Using the code in Section~\ref{subsec:laplace_equation_code}, this equation was solved using the Jacobi and Gauss-Seidel relaxation methods.
A $ 64 \times 64 $ grid was used with a convergence criteria of $ \left| u^{n+1} - u^{n} \right|_\text{max} < 10^{-8} $.
The results are shown in Figure~\ref{fig:laplace}.
The solution looks as we expect: the boundary conditions are met, and the function makes a smooth transition in between, almost like a sheet of rubber stretched over a bumpy, square frame.

The two solutions are indistinguishable from each other, which is to be expected since we used the same convergence criteria for both.
To reach this level of convergence, the Jacobi method took 3329 iterations and the Gauss-Seidel method took 3638 iterations.
So the Gauss-Seidel took slightly longer to reach the same level of convergence.
Note that this is quite a large number of iterations considering that the spatial resolution is only 64 points on each axis.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Laplace.pdf}
    \caption{Solutions to the Laplace equation as determined by the Jacobi and Gauss-Seidel relaxation methods.}
    \label{fig:laplace}
\end{figure*}

\subsection{Examples of PDE's}
\label{subsec:examples_of_pdes}

Here are some examples of partial differential equations which are important in a number of applications:
\begin{itemize}
\item
    Poisson's equation (2\textsuperscript{nd} order):
    \begin{align}
        \nabla^2 u &= \rho
    \end{align}
    where
    \begin{align}
        \nabla^2 &= \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}
    \end{align}
    
\item
    Laplace's equation (2\textsuperscript{nd} order):
    \begin{align}
        \nabla^2 u &= 0
    \end{align}
\item
    Wave equation (2\textsuperscript{nd} order):
    \begin{align}
        v^2 \nabla^2 u &= \frac{\partial^2 u}{\partial t^2}
    \end{align}
\item
    Heat equation in one dimension (2\textsuperscript{nd} order):
    \begin{align}
        \alpha \frac{\partial^2 u}{\partial x^2} &= \frac{\partial u}{\partial t}
    \end{align}
\item
    Complex Ginzburg-Landau equation (2\textsuperscript{nd} order):
    \begin{align}
        \frac{\partial u}{\partial t} &= (1 + i \alpha) \nabla^2 u + u - (1 + i \beta) |u|^2 u 
    \end{align}
\item
    Korteweg-de Vries equation (3\textsuperscript{rd} order):
    \begin{align}
        \frac{\partial u}{\partial t} + \frac{\partial^3 u}{\partial x^3} - 6 u \frac{\partial u}{\partial x} &= 0
    \end{align}
\end{itemize}

\subsection{Classification of PDE's}
\label{subsec:classification_of_pde_s}

Consider a PDE of the form
\begin{align}
    a u_{xx} + b u_{xy} + c u_{yy} + d u_{x} + e u_{y} + fu &= 0.
\end{align}
We define the discriminant as
\begin{align}
    \Delta &= b^2 - 4 a c.
\end{align}
We can then classify the equation as
\begin{itemize}
\item
    Elliptic if $ \Delta < 0 $
\item
    Parabolic if $ \Delta = 0 $
\item
    Hyperbolic if $ \Delta > 0 $
\end{itemize}

For
\begin{align}
    u_{tt} - 2 u_{xx} &= 0
\end{align}
we have $ \Delta = - 4 (1) (-2) > 0 $, so this equation is hyperbolic.

For 
\begin{align}
    u_{xx} + u_{yy} &= 0
\end{align}
we have $ \Delta = -4 (1) (1) < 0 $, so this equation is elliptic.

For
\begin{align}
    u_t - u_{xx} &= 0
\end{align}
we have $ \Delta = 0 $ so this equation is parabolic.

\subsection{Finite difference schemes}
\label{subsec:finite_difference_schemes}

If we take a function $ u(x,t) $ and discretize it using
\begin{align}
    u^{n}_{j} &= u(x_0 + j \Delta x, t_0 + n \Delta t)
\end{align}
then we can approximate differential equations in $ u(x,t) $ by difference equations in $ u^n_j $.
Table~\ref{tab:finite_difference} shows various finite difference approximations for the spatial and temporal derivatives of $ u(x,t) $.
The order of the approximation describes how quickly the approximation converges as the difference approaches zero.
For example, a second-order finite-difference approximation to a spatial derivative will have an error on the order of $ (\Delta x)^2 $ while a first order approximation will have an error on the order of $ \Delta x $.

\begin{table}
    \begin{tabular}{cccc}
        \toprule
        Derivative & FD approximation & Type & Order
        \\
        \midrule
        $ \dfrac{\partial u}{\partial x} $ & $ \dfrac{u^{n}_{j+1} - u^{n}_{j}}{\Delta x} $ & Forward & 1\textsuperscript{st} 
        \\[2.2ex]
        $ \dfrac{\partial u}{\partial x} $ & $ \dfrac{u^{n}_{j} - u^{n}_{j-1}}{\Delta x} $ & Backward & 1\textsuperscript{st}
        \\[2.2ex]
        $ \dfrac{\partial u}{\partial x} $ & $ \dfrac{u^{n}_{j+1} - u^{n}_{j-1}}{2 \Delta x} $ & Central & 2\textsuperscript{nd}
        \\[2.2ex]
        $ \dfrac{\partial^2 u}{\partial x^2} $ & $ \dfrac{u^{n}_{j+1} - 2 u^{n}_{j} + u^{n}_{j-1}}{(\Delta x)^2} $ & Symmetric & 2\textsuperscript{nd}
        \\[2.2ex]
        $ \dfrac{\partial u}{\partial t} $ & $ \dfrac{u^{n+1}_{j} - u^{n}_{j}}{\Delta t} $ & Forward & 1\textsuperscript{st}
        \\[2.2ex]
        $ \dfrac{\partial u}{\partial t} $ & $ \dfrac{u^{n}_{j} - u^{n-1}_{j}}{\Delta t} $ & Backward & 1\textsuperscript{st}
        \\[2.2ex]
        $ \dfrac{\partial u}{\partial t} $ & $ \dfrac{u^{n+1}_{j} - u^{n-1}_{j}}{2\Delta t} $ & Central & 2\textsuperscript{nd}
        \\[2.2ex]
        $ \dfrac{\partial^2 u}{\partial t^2} $ & $ \dfrac{u^{n+1}_{j} - 2 u^{n}_{j} + u^{n-1}_{j}}{(\Delta t)^2} $ & Symmetric & 2\textsuperscript{nd}
        \\[2.2ex]
        \bottomrule
    \end{tabular}
    \caption{Table of finite difference approximations to the derivatives of $ u(x,t) $.}
    \label{tab:finite_difference}
\end{table}

As an example, we will estimate the derivative of $ u(x) = x^2 $ using finite differences.
Analytically, we know that $ u'(x) = 2x $, and so $ u'(3) = 6 $.

Applying a forward difference we get
\begin{align}
    u'(3) &\approx \frac{u(3 + \Delta x) - u(3)}{\Delta x}
    \\
    u'(3) &\approx \frac{(3 + \Delta x)^2 - (3)^2}{\Delta x}
    \\
    u'(3) &\approx \frac{9 + 6 \Delta x + (\Delta x)^2 - 9}{\Delta x}
    \\
    u'(3) &\approx 6 + \Delta x
\end{align}

So we can see that with $ \Delta x = 0.1 $ we will have
\begin{align}
    u'(3) &\approx 6.1
\end{align}
and with $ \Delta x = 0.05 $ we will have
\begin{align}
    u'(3) &\approx 6.05
\end{align}
So we can see that this approaches the correct value as $ \Delta x \to 0 $.

Applying a central difference we get
\begin{align}
    u'(3) &\approx \frac{u(3 + \Delta x) - u(3 - \Delta x)}{2 \Delta x}
    \\
    u'(3) &\approx \frac{(3 + \Delta x)^2 - (3 - \Delta x)^2}{2 \Delta x}
    %\\
    %u'(3) &\approx \frac{9 + 6 \Delta x + (\Delta x)^2 - (9 - 6 \Delta x + (\Delta x)^2 )}{2 \Delta x}
    \\
    u'(3) &\approx \frac{9 + 6 \Delta x + (\Delta x)^2 - 9 + 6 \Delta x - (\Delta x)^2 }{2 \Delta x}
    \\
    u'(3) &\approx 6
\end{align}

So the second order approximation is actually exactly correct.
Whether we use $ \Delta x = 0.1 $ or $ \Delta x = 0.05 $, we will get the exact answer which is $ u'(3) = 6 $.
So we can see that the central difference approximation is more accurate than the forward difference approximation.

In this case we got better than second-order accurate (we got perfectly accurate), but in general the central difference approximation will be at worst second-order accurate.
This is better than the forward difference approximation which is at worst first-order accurate.

\subsection{Advection equation schemes}
\label{subsec:advection_equation_schemes}

There are a number of finite difference schemes which can be used to solve the advection equation
\begin{align}
    u_t &= c u_x
\end{align}
with $ c > 0 $.
They are most conveniently formulated in terms of the Courant number
\begin{align}
    r &= \frac{c \Delta t}{\Delta x}
\end{align}

A number of explicit and implicit schemes are shown in Table~\ref{tab:advection_schemes}.
Note that the Forward Euler scheme requires $ r = 0 $ for stability, which means the scheme is unconditionally unstable.
We see that the implicit schemes are unconditionally stable: the Courant number can be as big as desired without making the scheme unstable.
This does not mean that large Courant numbers are the best choice for accuracy, of course, just that the solution will not blow up.

\begin{table*}[ht]
    \centering
    \begin{tabular}{cccc}
        \toprule
        Name & Expression for $ u^{n+1}_j $ & Order & CFL 
        \\
        \midrule
        Forward Euler & $ u^n_j + \frac{1}{2} r \left(u^{n}_{j+1} - u^{n}_{j-1}\right) $ & 1\textsuperscript{st} & $ r = 0 $ 
        \\[1.5ex]
        Upwind & $ u^n_j + r \left(u^{n}_{j} - u^{n}_{j-1}\right) $ & 1\textsuperscript{st} & $ |r| < 1 $ 
        \\[1.5ex]
        Leap-Frog & $ u^{n-1}_j + r \left(u^{n}_{j+1} - u^{n}_{j-1}\right) $ & 2\textsuperscript{nd} & $ |r| < 1 $ 
        \\[1.5ex]
    Lax-Wendroff & $ u^n_j + \frac{1}{2} r \left(u^{n}_{j+1} - u^{n}_{j-1}\right) + \frac{1}{2} r^2 \left(u^{n}_{j+1} - 2 u^n_j + u^{n}_{j-1}\right) $ & 2\textsuperscript{nd} & $ |r| < 1 $ 
        \\[1.5ex]
        \midrule
        Backward Euler & $ u^n_j + \frac{1}{2} r \left(u^{n+1}_{j+1} - u^{n+1}_{j-1}\right) $ & 1\textsuperscript{st} & $ |r| < \infty $ 
        \\[1.5ex]
        Crank-Nicolson & $ u^n_j + \frac{1}{4} r \left( u^{n+1}_{j+1} - u^{n+1}_{j-1} + u^{n}_{j+1} - u^{n}_{j-1}\right) $ & 2\textsuperscript{nd} & $ |r| < \infty $ 
        \\[1.5ex]
        \bottomrule
    \end{tabular}
    \caption{Finite difference schemes for the advection equation.}
    \label{tab:advection_schemes}
\end{table*}

\subsection{Implicit versus explicit schemes}
\label{subsec:implicit_versus_explicit_schemes}

Finite difference methods are usually specified by a set of update equations, which give relationships between the value of the function at different grid points.
We use these update equations along with initial/boundary conditions to solve for all the desired values.

In an explicit finite difference scheme, each of these update equations only involves \emph{one} unknown value (and any number of known values).
This is useful because it means that we can simply loop through the update equations and solve for the unknown value.
For example, in the forward Euler scheme for the advection equation, our update equations are
\begin{align}
    u^{n+1}_{j} = u^n_j + \frac{1}{2} r \left(u^{n}_{j+1} - u^{n}_{j-1}\right)
\end{align}
If we loop through $ n $ and $ j $ in the correct order, we will always know $ u^n_j $, $ u^n_{j+1} $, and $ u^{n}_{j-1} $; and the only unknown will be $ u^{n+1}_{j} $.
Thus, programming this scheme will be easy, and the updates will be quite fast computationally.

In an implicit scheme, we have more than one unknown per update equation.
This requires us to solve a number of simultaneous equations, which can be challenging and time-consuming depending on the system.
Methods like Crank-Nicolson may only require the solution of a tridiagonal matrix, which is not that hard to do quickly.
However, if non-linear update equations arise, solving the system of equations could be extremely challenging.

In general, implicit schemes tend to have nicer properties in terms of stability and convergence when compared to explicit schemes.
However, the fact that they are slow and difficult to implement hurts their utility in practise.

\subsection{Von Neumann stability analysis}
\label{subsec:von_neumann_stability_analysis}

Von Neumann stability analysis is used to determine whether a given finite difference scheme is stable.
An unstable scheme will ``blow up'' in the sense that small perturbations are amplified and grow without bound, giving a useless solution.
The idea of Von Neumann stability analysis is to plug in a complex exponential and watch how the scheme changes its amplitude.
For stability, we want to ensure that at each time step, the amplitude of the mode does not increase.

Complex exponentials represent a complete basis set for all possible solutions, so the amplitude of these complex exponentials is related to the total energy in the system.
If none of the amplitudes are growing, then the total energy cannot be growing, and the system is guaranteed to be stable.
If all of the amplitudes grow without bound, then the energy will grow without bound for any initial conditions and the system is guaranteed to be unstable.
In intermediate cases, the system may be unstable, but only for certain initial conditions.

Typically, we would do Von Neumann analysis and pick our scheme so that it is guaranteed to be stable.
For many cases (especially linear systems), Von Neumann stability analysis is straight-forward, and stability is guaranteed by a simple condition.
For more complicated cases like non-linear equations, stability analysis may be more complicated.

\subsection{Stability of Crank-Nicolson}
\label{subsec:stability_of_crank_nicolson}

As seen in Table~\ref{tab:advection_schemes}, the Crank-Nicolson update equation for the advection equation is
\begin{align}
    u^{n+1}_j - u^{n}_j - \frac{1}{4} r \left( u^{n+1}_{j+1} - u^{n+1}_{j-1} + u^{n}_{j+1} - u^{n}_{j-1}\right) = 0
\end{align}

We will use the ansatz $ \tilde{u}^n_j = z^n e^{i j \omega} $.
Note the shift properties of this ansatz:
\begin{align}
    \tilde{u}^{n}_{j+k} &= e^{i k \omega} \tilde{u}^{n}_{j} \label{eq:space_shift_property}
    \\
    \tilde{u}^{n+k}_{j} &= z^{k} \tilde{u}^{n}_{j} \label{eq:time_shift_property}
\end{align}

Using this, we get
\begin{align}
    \left[ z - 1 - \frac{1}{4} r \left( z e^{+i\omega} - z e^{-j\omega} + e^{+i\omega} - e^{-i\omega} \right) \right] \tilde{u}^{n}_{j} = 0
\end{align}
\begin{align}
    \left[ z - 1 - \frac{i}{2} r \left( z \sin(\omega) + \sin(\omega) \right) \right] \tilde{u}^{n}_{j} = 0
    \\
    \Big[ z \big( 2 - i r \sin(\omega) \big) - i r \sin(\omega) - 2 \Big] \tilde{u}^{n}_{j} = 0
\end{align}

Solving for the amplification factor $ z $ we get
\begin{align}
    z &= \frac{2 + i r \sin(\omega)}{2 - i r \sin(\omega)}
    \\
    |z|^2 &= \frac{4 + r^2 \sin^2(\omega)}{4 + r^2 \sin^2(\omega)} = 1
\end{align}

So the Crank-Nicolson scheme is unconditionally stable for the advection equation.

\section{FDs in the Numerical Recipes}
\label{sec:fds_in_the_numerical_recipes}

We apply various schemes to flux conservative PDEs of the type
\begin{align}
    \frac{\partial u(x,t)}{\partial t} + \frac{\partial f(u)}{\partial x} &= 0
\end{align}

The forward-time centred-space discretization is
\begin{align}
    \frac{u^{n+1}_j - u^n_j}{\Delta t} + \frac{f\left(u^n_{j+1}\right) - f\left(u^n_{j-1}\right)}{\Delta x} &= 0
\end{align}
This method is almost never used because it is unconditionally unstable for $ f(u) = c u $.

The Lax method is given by
\begin{align}
    u^{n+1}_j  &= \frac{1}{2} \left( u^n_{j+1} + u^n_{j-1} \right) - \frac{\Delta t}{2 \Delta x} \left( f\left( u^n_{j+1} \right) - f\left( u^n_{j-1} \right) \right)
\end{align}
The difference compared to the $ FTCS $ is that we have replaced $ u^n_j $ with the average of the two adjacent values.

The Upwind method is given by
\begin{align}
u^{n+1}_j &= u^n_j - \frac{\Delta t}{\Delta x} \begin{cases} f(u^n_j) - f(u^n_{j-1}) & \text{for } \frac{\partial f(u)}{\partial u} > 0 \\ f(u^n_{j+1}) - f(u^n_j) & \text{for } \frac{\partial f(u)}{\partial u} < 0 \end{cases}
\end{align}

The Lax-Wendroff method is given by
\begin{align}
    u^{n+1/2}_{j+1/2} &= \frac{1}{2} \left( u^n_{j+1} + u^n_j \right) - \frac{\Delta t}{\Delta x} \left[ f \left( u^n_{j+1} \right) - f\left( u^n_j \right) \right]
    \\
    u^{n+1}_{j} &= u^n_j - \frac{\Delta t}{\Delta x} \left[ f \left( u^{n+1/2}_{j+1/2} \right) - f\left( u^{n+1/2}_{j-1/2} \right) \right]
\end{align}

\section{Popular cases}
\label{sec:popular_cases}

\subsection{The advection equation}
\label{subsec:the_advection_equation}

Here we use a number of finite difference schemes to solve the advection equation
\begin{align}
    \frac{\partial u}{\partial t} &= c \frac{\partial u}{\partial x}
\end{align}

The exact analytical solution can be determined using the method of characteristics, which is essentially a change of variables.
If we define $ \xi = x - ct $ and $ \eta = x + ct $, then
\begin{align}
    \frac{\partial u}{\partial t} &= \frac{\partial u}{\partial \xi} \frac{\partial \xi}{\partial t} + \frac{\partial u}{\partial \eta} \frac{\partial \eta}{\partial t}
    \\
    \frac{\partial u}{\partial t} &= - c \frac{\partial u}{\partial \xi} + c \frac{\partial u}{\partial \eta}
\end{align}
and
\begin{align}
    \frac{\partial u}{\partial x} &= \frac{\partial u}{\partial \xi} \frac{\partial \xi}{\partial x} + \frac{\partial u}{\partial \eta} \frac{\partial \eta}{\partial x}
    \\
    \frac{\partial u}{\partial x} &= \frac{\partial u}{\partial \xi} + \frac{\partial u}{\partial \eta}
\end{align}

Plugging these into the advection equation we get
\begin{align}
    -c \frac{\partial u}{\partial \xi} + c \frac{\partial u}{\partial \eta} &= c \frac{\partial u }{\partial \xi} + c \frac{\partial u}{\partial \eta}
\end{align}
and simplifying, this becomes
\begin{align}
    \frac{\partial u}{\partial \xi} &= 0
\end{align}

This tells us that $ u $ is \emph{not} a function of $ \xi $, and thus the solution is of the form
\begin{align}
    u(x,t) &= u(\eta) = u(x + ct)
\end{align}

In particular, if the initial condition is given by
\begin{align}
    u(x,0) &= u_0(x)
\end{align}
then the solution for any time is just
\begin{align}
    u(x,t) &= u_0(x + ct)
\end{align}
and we see that the shape of $ u_0 $ is maintained, it is simply shifted along the $ x $ axis with velocity $ c $.

As an example, suppose $ u_0 $ is some sort of pulse.
If $ c > 0 $, then the pulse will move to the left with time.
If $ c < 0 $, then the pulse will move to the right with time.

We will now solve the advection equation in a super-duper number of different ways.
We will use four different initial conditions:
\begin{align}
    u_1(x,0) &= \sin(2x)
    \\
    u_2(x,0) &= \begin{cases} 0 & \text{if } x < 0 \\ 1 & \text{if } x \geq 0 \end{cases}
    \\
    u_3(x,0) &= \begin{cases} 0 & \text{if } x < 0 \\ \dfrac{1}{\Delta x} & \text{if } x = 0 \\ 0 & \text{if } x > 0 \end{cases}
    \\
    u_4(x,0) &= \exp\left( -4 x^2 \right)
\end{align}
For each initial condition, we will apply 7 different sets of numerical parameters, seen in Table~\ref{tab:advection_parameters}.
For each of these sets of parameters, we use 4 of the finite difference schemes from Table~\ref{tab:advection_schemes}: Forward Euler, Lax-Wendroff, Backward Euler, and Crank-Nicolson.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Trial & $ c $ & $ \Delta x $ & $ \Delta t $ & $ r $
        \\
        \midrule
        1 & 0.5 & 0.04 & 0.02 & 0.25
        \\
        2 & 0.5 & 0.02 & 0.02 & 0.5
        \\
        3 & 0.5 & 0.0137 & 0.02 & 0.728
        \\
        4 & 0.5 & 0.0101 & 0.02 & 0.99
        \\
        5 & 0.5 & 0.0099 & 0.02 & 1.11
        \\
        6 & 0.5 & 0.02 & 0.01 & 0.25
        \\
        7 & 0.5 & 0.02 & 0.04 & 1
        \\
        \bottomrule
    \end{tabular}
    \caption{FD parameters for the advection equation.}
    \label{tab:advection_parameters}
\end{table}

The code in Section~\ref{subsec:advection_equation_code} performs all these solutions.
The resulting solutions are plotted on the pages just before the Code section (Section~\ref{sec:code}).
To go through all these plots in detail would be tedious, but we can see some general results.

In terms of stability, we see the following.
\begin{itemize}
\item
    The Forward Euler almost always leads to unstable or wildly-inaccurate solutions.
    This is why the Forward Euler method should never be used for advection problems.
\item
    The Lax-Wendroff method is stable for all the tested values of $ r $, except for in trial 5 where $ r $ is slightly greater than 1.
    In that case, we begin to see instability (though it is not as severe as the Forward Euler).
    Note that Lax-Wendroff is even stable for the ``edge'' cases where $ r = 0.99 $ and $ r = 1 $.
\item
    The other two schemes are implicit schemes, and we see that they are always stable.
\end{itemize}

In terms of parameters, we see the following.
\begin{itemize}
\item
    Increasing the grid density improves the solution.
    For example, in trial 6 the grid is twice as dense as in trial 1 (and the Courant number is the same).
    We can see that the solutions for trial 6 are more accurate than the solutions for trial 1.
    Particularly look at initial conditions 2 and 3, where the sharp transitions make the accuracy more noticeable.
\item
    Values of $ r $ which are close to 1 (but not over!) appear to give the best results.
    For example, compare the Lax-Wendroff solution for the third initial condition between trials 6 and 7.
    We see that the trial 7 solution is much better, even through the grid is actually sparser compared with trial 6.
    This is simply because the Courant number was chosen well.
\end{itemize}

In terms of schemes, we see the following.
\begin{itemize}
\item
    All methods except for forward Euler perform quite well for initial conditions 1 and 4, where there are no sharp transitions.
\item
    The backward Euler method tends to ``smear out'' the sharp transitions seen in initial conditions 2 and 3.
    However, it does not introduce and oscillatory behaviour near the transitions.
    This is characterisitc of a first-order scheme.
\item
    The Crank-Nicolson and Lax-Wendroff methods both acheive much sharper transitions for initial conditions 2 and 3.
    However, they acheive this by introducing oscillations near the edges of the transition.
    This is characteristic of a second-order scheme.
\end{itemize}

In summary:
\begin{itemize}
\item
    Forward Euler should \emph{never} be used for advection problems.
\item
    Lax-Wendroff is comparable to Crank-Nicolson in terms of accuracy, and it has the advantage of being an easy-to-implement explicit scheme.
    Its downsides are that it introduces oscillations near sharp transitions and it has the potential for instability if the CFL condition is not met.
\item
    Backward Euler does not introduce any oscillations near sharp transitions, which may be an important feature when modelling fluid dynamics.
    It is also unconditionally stable, though its results are not as accurate as Lax-Wendroff or Crank-Nicolson.
\item
    Crank-Nicolson is quite accurate and unconditionally stable.
    Its downsides are that it is an implicit scheme (difficult to implement) and it introduces oscillations near sharp transitions.
\end{itemize}

\subsection{The diffusion equation}
\label{subsec:the_diffusion_equation}

In this section we solve the sourceless diffusion equation
\begin{align}
    \frac{\partial u}{\partial t} - \beta \frac{\partial^2 u}{\partial x^2} &= 0
\end{align}
on the interval $ x \in [0,1] $ with initial condition
\begin{align}
    u(x,0) &= \sin(\pi x)
\end{align}
and boundary conditions
\begin{align}
    u(0, t) &= u(1, t) = 0
\end{align}

It is easy to verify by direct substitution that the analytical solution is
\begin{align}
    u(x, t) &= e^{-\beta \pi^2 t} \sin(\pi x)
\end{align}

We will solve this by finite differences, using a first order forward difference for the temporal derivative and a second order symmetric difference for the spatial derivative.
We get
\begin{align}
    \frac{u^{n+1}_j - u^n_j}{\Delta t} - \beta \frac{u^{n}_{j+1} - 2 u^n_j + u^{n}_{j-1}}{(\Delta x)^2} &= 0
\end{align}
which gives
\begin{align}
    u^{n+1}_j &= u^n_j + r \left( u^{n}_{j+1} - 2 u^n_j + u^{n}_{j-1} \right)
\end{align}
where
\begin{align}
    r &= \frac{\beta \Delta t}{(\Delta x)^2}
\end{align}

To analyze the stability of this scheme, we will use the ansatz $ \tilde{u}^n_j = z^n e^{i j \omega} $, and we will again use the space and time shift properties~\eqref{eq:space_shift_property} and~\eqref{eq:time_shift_property}.
Using this properties and plugging this ansatz into the finite difference scheme, we arrive at
\begin{align}
    z \tilde{u}^n_j &= \tilde{u}^n_j + r \left( e^{+i \omega} \tilde{u}^n_j - 2 \tilde{u}^n_j + e^{-i\omega} \tilde{u}^n_j \right)
    \\
    z \tilde{u}^n_j &= \left( r e^{+i\omega} + r e^{-i\omega} + 1 - 2 r \right) \tilde{u}^n_j
\end{align}
So the amplification factor is
\begin{align}
    z &= r e^{+i\omega} + r e^{-i\omega} + 1 - 2 r
    \\
    z &= 2 r \cos(\omega) + 1 - 2 r
\end{align}
Since $ \cos(\omega) $ ranges between $ -1 $ and $ 1 $, the amplification factor will be between $ 1 - 4r $ and $ 1 $.

So, for stability, we need
\begin{align}
    -1 < 4r &- 1 < 1
    \\
    0 < &4r < 2
    \\
    0 < &r < \frac{1}{2}
\end{align}

Thus if $ \beta > 0 $ (which it must be, since otherwise our analytical equation is unstable), our stability condition is
\begin{align}
    \frac{\beta \Delta t}{(\Delta x)^2} &< \frac{1}{2}
\end{align}
Or, since $ \Delta x $ is usually determined by the spatial variation in the diffusion coefficient and the initial conditions, our stability condition can be written as
\begin{align}
    \Delta t &< \Delta t_\text{max} = \frac{(\Delta x)^2}{2 \beta}
\end{align}

The FD solution to this diffusion problem was found for $ \Delta t = 0.9 \Delta t_\text{max} $, 300 time steps, 51 grid points, and $ \beta = 1 $.
The results are shown in Figure~\ref{fig:diffusion}.
Not surprisingly, we see that the solution gets more ``diffuse'' as time progresses.
We can also see that the numerical solution is effectively indistinguishable from the analytical solution.
So we have found an effective finite-difference method for solving the diffusion equation.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Diffusion.pdf}
    \caption{Numerical and analytical solutions to the diffusion equation: full solution.}
    \label{fig:diffusion}
\end{figure*}

In Figure~\ref{fig:diffusion2}, we see the initial and final conditions for both the analytical and finite-difference solutions.
We see again that the numerical solution is in excellent agreement with the analytical solution.
We see the expected behaviour of the diffusion equation: the sine pulse simply decreases in amplitude exponentially over time.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Diffusion2.pdf}
    \caption{Numerical and analytical solutions to the diffusion equation: initial and final states.}
    \label{fig:diffusion2}
\end{figure*}

\subsection{The advection-diffusion equation}
\label{subsec:the_advection_diffusion_equation}

In this section, we solve the combined advection-diffusion equation
\begin{align}
    \frac{\partial u}{\partial t} + c \frac{\partial u}{\partial x} &= \beta \frac{\partial^2 u}{\partial x^2}
\end{align}

We discretize this equation by replacing $ \partial/\partial t $ with a forward difference, $ \partial/\partial x $ with a backward difference, and $ \partial^2/\partial x^2 $ with a symmetric difference:
\begin{align}
    \frac{u^{n+1}_j - u^n_j}{\Delta t} + c \frac{u^n_j - u^n_{j-1}}{\Delta x} &= \beta \frac{u^n_{j+1} - 2 u^n_j + u^n_{j-1}}{(\Delta x)^2}
\end{align}
This gives
\begin{align}
    u^{n+1}_j &= u^n_j - r \left(u^n_j - u^n_{j-1}\right) + s \left(u^n_{j+1} - 2 u^n_j + u^n_{j-1} \right)
    \\
    u^{n+1}_j &= s u^n_{j+1} + (1 - r - 2s) u^n_j + (r + s) u^n_{j-1}
\end{align}
where
\begin{align}
    r &= \frac{c \Delta t}{\Delta x} \quad \text{and} \quad s = \frac{\beta \Delta t}{(\Delta x)^2}
\end{align}

To analyze the stability of this scheme, we will again use the ansatz $ \tilde{u}^n_j = z^n e^{i j \omega} $, and we will again use the space and time shift properties~\eqref{eq:space_shift_property} and~\eqref{eq:time_shift_property}.
Using this properties and plugging this ansatz into the finite difference scheme, we arrive at
\begin{align}
    z \tilde{u}^n_j &= \left[ s e^{+i\omega} + (1 - r - 2s) + (r+s) e^{-i\omega} \right] \tilde{u}^n_j
\end{align}
So the amplification factor is
\begin{align}
    z &= s e^{+i\omega} + (1 - r - 2s) + (r+s) e^{-i\omega}
    \\
    z &= 2 s \cos(\omega) + (1 - r - 2s) + r \big[ \cos(\omega) - i \sin(\omega) \big]
    \\
    z &= 1 + (r + 2 s) \big[\cos(\omega) - 1 \big] - i r \sin(\omega)
    \\
    |z|^2 &= \Big( 1 + (r + 2 s) \big[\cos(\omega) - 1 \big] \Big)^2 + r^2 \sin^2(\omega)
\end{align}
To simplify the expression, let $ \alpha = r + 2s $ and $ c_\omega = \cos(\omega) $.
Then
\begin{align}
    |z|^2 &= \Big( 1 + \alpha (c_\omega-1) \Big)^2 + r^2 \left( 1 - c_\omega^2 \right)
\end{align}
Since $ r^2 \leq \alpha^2 $ by definition of $ \alpha $ and $ 1 - c_\omega^2 > 0 $, we have
\begin{align}
    |z|^2 &\leq \Big( 1 + \alpha (c_\omega-1) \Big)^2 + \alpha^2 \left( 1 - c_\omega^2 \right)
    %\\
    %|z|^2 &\leq 1 + 2 \alpha (c - 1) + \alpha^2 (c-1)^2 + \alpha^2 (1 + c)(1 - c)
    %\\
    %|z|^2 &\leq 1 + 2 \alpha (c - 1) + \alpha^2 (1-c) (1 - c + 1 + c)
    %\\
    %|z|^2 &\leq 1 + 2 (c - 1) \left( \alpha - \alpha^2 \right)
\end{align}
after simplifying, this becomes
\begin{align}
    |z|^2 &\leq 1 + 2 (1 - c_\omega) \left( \alpha^2 - \alpha \right)
\end{align}
Now, since $ -1 \leq c_\omega \leq 1 $, we have $ 0 \leq 1 - c_\omega \leq 2 $, which means
\begin{align}
    |z|^2 &\leq 1 + 4 \left( \alpha^2 - \alpha \right)
\end{align}
So to have $ |z|^2 \leq 1 $ we clearly must have
\begin{align}
    \alpha^2 - \alpha &\leq 0
\end{align}
which means that
\begin{align}
    \alpha &\leq 1
    \\
    r + 2 s &\leq 1
    \\
    \frac{c \Delta t}{\Delta x} + 2 \frac{\beta \Delta t}{(\Delta x)^2} &\leq 1
    \\
    \Delta t \left( \frac{c \Delta x + 2 \beta}{(\Delta x)^2} \right) &\leq 1
\end{align}

So finally we have our stability criterion:
\begin{align}
    \Delta t &\leq \Delta t_\text{max} = \frac{(\Delta x)^2}{c \Delta x + 2 \beta} \label{eq:lax_wendroff_stability}
\end{align}

We now solve this FD equation for $ c = 0.5 $, $ \beta = 0.1 $, $ 101 $ grid points, $ \Delta t = 0.9 \Delta t_\text{max} $.
We solve the equation on the domain $ 0 \text{ m} \leq x \leq 60 \text{ m} $ and $ 0 \text{ s} \leq t \leq 57 \text{ s} $ with initial conditions
\begin{align}
    u(x, 0) &= \exp \left( \frac{(x - 12 \text{ m})^2}{18 \text{ m}^2} \right)
\end{align}
Note that under this set of parameters, it takes 88 time steps to move from $ t = 0 $ s to $ t = 57 $ s.

The results are plotted in Figure~\ref{fig:ad_combined}.
We see the expected behaviour.
The pulse moves $ 28.15 $ m to the right over 57 s.
This is in agreement with the advection equation, which predicts that the pulse should move to the right at $ 0.5 $ m/s.
We also see the pulse spread out over time: its standard deviation changes from 3 m to 5.38 m, which demonstrates the diffusion part of the advection-diffusion equation.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/AD_combined.pdf}
    \caption{Solution to the combined advection-diffusion equation.}
    \label{fig:ad_combined}
\end{figure*}

\subsection{Conservative and non-conservative forms}
\label{subsec:conservative_and_non_conservative_forms}

The inviscid 1D Burger's equation is
\begin{align}
    \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} &= 0
\end{align}
This is similar to the advection equation seen earlier, except that the velocity is replaced by $ u $.
This makes it a non-linear equation.
We will not simply see a shift of the initial wave in space; there can be a transfer of energy between Fourier modes due to the non-linearity.

We can rewrite this equation in conservative form
\begin{align}
    \frac{\partial u}{\partial t} + \frac{1}{2} \frac{\partial u^2}{\partial x} &= 0
    \\
    \frac{\partial u}{\partial t} + \frac{\partial f(u)}{\partial x} &= 0
\end{align}
where
\begin{align}
    f(u) &= \frac{1}{2} u^2
\end{align}
This is known as the conservative form of the equation.


We will discretize this equation in both its conservative an non-conservative forms.

The upwind scheme for the non-conservative form is
\begin{align}
u^{n+1}_j &= u^n_j - \frac{\Delta t}{\Delta x} u^n_j \begin{cases} u^n_j - u^n_{j-1} & \text{if } u^n_j > 0 \\ u^n_{j+1} - u^n_j & \text{if } u^n_j < 0 \end{cases}
\end{align}

The upwind scheme for the conservative form is
\begin{align}
    u^{n+1}_j &= u^n_j - \frac{\Delta t}{\Delta x} \begin{cases} f^n_j - f^n_{j-1} & \text{if } u^n_j > 0 \\ f^n_{j+1} - f^n_j & \text{if } u^n_j < 0 \end{cases}
    \\
    f^n_j &= \frac{1}{2} \left( u^n_j \right)^2
\end{align}

The Lax-Wendroff scheme for the non-conservative form is
\begin{align}
    u^{n+1}_j &= u^n_j + \frac{1}{2} \frac{\Delta t}{\Delta x} u^n_j \left(u^{n}_{j+1} - u^{n}_{j-1}\right) + \nonumber
    \\
    & \quad + \frac{1}{2} \left( \frac{\Delta t}{\Delta x} u^n_j \right)^2 \left(u^{n}_{j+1} - 2 u^n_j + u^{n}_{j-1}\right) 
\end{align}

The Lax-Wendroff scheme for the conservative form is
\begin{align}
    u^{n+1/2}_{j+1/2} &= \frac{1}{2} \left( u^n_{j+1} + u^n_{j} \right) - \frac{\Delta t}{2 \Delta x} \left( f^n_{j+1} - f^n_{j} \right)
    \\
    f^{n+1/2}_{j+1/2} &= \frac{1}{2} \left( u^{n+1/2}_{j+1/2} \right)^2
    \\
    u^{n+1}_j &= u^n_j - \frac{\Delta t}{\Delta x} \left( f^{n+1/2}_{j+1/2} - f^{n+1/2}_{j-1/2} \right)
    \\
    f^{n+1}_j &= \frac{1}{2} \left( u^{n+1}_j \right)^2
\end{align}

For stability of these schemes, we need
\begin{align}
    \frac{\Delta t}{\Delta x} \left| u^n_j \right| &\leq 1
\end{align}
Alternatively, the condition on the time-step is
\begin{align}
    \Delta t &\leq \Delta t_\text{max} = \frac{\Delta x}{\left| u^n_j \right|}
\end{align}
This suggests that we should adaptively pick a time step using
\begin{align}
    \Delta t &\leq \Delta t_\text{max} = \frac{\Delta x}{\left| u^n_j \right|_\text{max}} \leq \frac{\Delta x}{\left| u^n_j \right|}
\end{align}

Using these four schemes, we solve the Burger's equation on $ x \in [-1, 1] $ for initial condition
\begin{align}
    u(x, 0) &= \frac{2}{3} \exp \left( - 12 x^2 \right)
\end{align}
and boundary conditions
\begin{align}
    u(1, t) &= u(1, 0)
    \\
    u(-1, t) &= u(-1, 0)
\end{align}

The solutions from all four schemes are plotted in Figure~\ref{fig:burgers_inviscid}.
In all of the plots, we can see the formation of a shock front over time.
We see that the conservative equations do a better job of modelling the shock front.
Conservation is a fundamental physical property, and these equations preserve that, so it is not surprising that they do a better job of modelling the physics.

We also see that the upwind scheme is better than the Lax-Wendroff for fluid dynamics.
The Lax-Wendroff is second-order accurate, so it should be more accurate mathematically.
However, the Lax-Wendroff introduces oscillations which are not present in the actual physical system.
In fluid dynamics, these oscillations could lead a researcher to incorrect conclusions, so it is better to go with the upwind.
The upwind is less accurate mathematically, but preserves the rough ``shape'' of the solution better.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Burgers_inviscid.pdf}
    \caption{Solutions to the inviscid Burger's equation with different numerical schemes.}
    \label{fig:burgers_inviscid}
\end{figure*}

\section{General applications}
\label{sec:general_applications}

\subsection{Fluid mechanics}
\label{subsec:fluid_mechanics}

Here we solve the general Burger's equation
\begin{align}
    \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} - \nu \frac{\partial^2 u}{\partial x^2} &= 0
\end{align}
This is similar to the Burger's equation above, but we have added a viscous diffusion term.

Writing the Burger's equation in conservative form, we get
\begin{align}
    \frac{\partial u}{\partial t} + \frac{\partial f(u)}{\partial x} &= 0
\end{align}
with
\begin{align}
    f(u, \nu) &= \frac{1}{2} u^2 - \nu \frac{\partial u}{\partial x}
\end{align}

For an advection-diffusion equation such as this, the Lax-Wendroff method is \emph{second} order.
Using Equation~\ref{eq:lax_wendroff_stability} and replacing the velocity $ c $ with the maximum velocity in the system, we get the stability criterion:
\begin{align}
    \Delta t &\leq \Delta t_\text{max} = \frac{(\Delta x)^2}{|u_j^n|_\text{max} \Delta x + 2 \beta}
\end{align}

The non-conservative Lax-Wendroff scheme for the non-conservative form of the general Burger's equation is
\begin{align}
    u^{n+1}_j &= \frac{1}{2} \left( u^n_j \frac{\Delta t}{\Delta x} + \left( u^n_j \frac{\Delta t}{\Delta x} \right)^2 + 2 \nu \frac{\Delta t}{(\Delta x)^2} \right) u^n_{j-1} + \nonumber
    \\ 
    & \quad + \left( 1 -  \left( u^n_j \frac{\Delta t}{\Delta x} \right)^2  - 2 \nu \frac{\Delta t}{(\Delta x)^2} \right) u^n_j + \nonumber
    \\ 
    & \quad + \frac{1}{2} \left( -u^n_j \frac{\Delta t}{\Delta x} + \left( u^n_j \frac{\Delta t}{\Delta x} \right)^2 + 2 \nu \frac{\Delta t}{(\Delta x)^2} \right) u^n_{j+1}
\end{align}

The Lax-Wendroff scheme for the conservative form is
\begin{align}
    u^{n+1/2}_{j+1/2} &= \frac{1}{2} \left( u^n_{j+1} + u^n_{j} \right) - \frac{\Delta t}{2 \Delta x} \left( f^n_{j+1} - f^n_{j} \right)
    \\
    f^{n+1/2}_{j+1/2} &= \frac{1}{2} \left( u^{n+1/2}_{j+1/2} \right)^2 - \nu \frac{u^n_{j+1} - u^n_{j-1}}{\Delta x}
    \\
    u^{n+1}_j &= u^n_j - \frac{\Delta t}{\Delta x} \left( f^{n+1/2}_{j+1/2} - f^{n+1/2}_{j-1/2} \right)
    \\
    f^{n+1}_j &= \frac{1}{2} \left( u^{n+1}_j \right)^2 - \nu \frac{u^{n+1}_{j+1} - u^{n+1}_{j-1}}{\Delta x}
\end{align}

The Burger's equation was solved using the code in Section~\ref{subsec:burgers_general_equation_code}.
The results are shown in Figure~\ref{fig:burgers_general}.
In both cases we see clearly the formation of two shock fronts as time progresses.
The cases are almost indistinguishable from one another because the Lax-Wendroff method is originally derived using the flux-conservative approach.
In the end, the non-conservative Lax-Wendroff ends up being very similar to the conservative.

Compared to the inviscid Burger's equation, we see a great deal of improvement.
For the Burger's equation with viscosity, we see that there are no spurious oscillations; they have been damped out by the diffusion term.
This actually gives us better stability because spurious oscillations have the potential to ``blow up'' with a non-linear equation like the Burger's.
With the spurious oscillations damped out, we have a solution which is more stable and more accurate to reality.
In some sense, the viscosity term makes the equation easier to solve.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{../Plots/Burgers_general.pdf}
    \caption{Solutions to the general Burger's equation using the Lax Wendroff method.}
    \label{fig:burgers_general}
\end{figure*}


\subsection{Quantum mechanics}
\label{subsec:quantum_mechanics}

In this section we solve a form of the time-independent Schrodinger equation
\begin{align}
    \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} - \alpha u &= 0
\end{align}
with the boundary conditions
\begin{align}
    u(0,y) = u(1, y) = u(x, 0) = u(x, 1) = 1
\end{align}

This equation was solved using the direct method and the weighted Jacobi method, using the code in Section~\ref{subsec:quantum_mechanics_code}.
The equation was solved for both $ \alpha = 1 $ and $ \alpha = 1000 $, and the results are shown in Figures~\ref{fig:direct_method}, \ref{fig:weighted_jacobi_1}, \ref{fig:weighted_jacobi_1000}.

Focusing first on the $ \alpha = 1 $ case, we can see that both methods give what we expect: a well shape which is only slightly perturbed from the pure-Laplace equation solution of $ u(x,y) = 1 $.
We note that when $ \omega $ is chosen too small (e.g., the $ \omega = 0.01 $ case), the Jacobi method does not converge quickly to the ``exact'' solution of the direct approach.
However, with an appropriately-chosen $ \omega $, we can get results which match those of the direct method.
We see similar things for the $ \alpha = 1000 $ case, except here we are significantly perturbed from the pure-Laplace equation, and our well is quite deep.

It is worth noting that the weighted Jacobi method is both faster than the direct method (with an appropriately chosen $ \omega $) and it is \emph{significantly} easier to set up and program.
For this reason, the weighted Jacobi method (or some other relaxation method) should almost always be preferred over the direct method.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Direct.pdf}
    \caption{Solution to the time-independent Schrodinger equation using the direct method.}
    \label{fig:direct_method}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Weighted_Lax1_01.pdf}
    \caption{Solution to the time-independent Schrodinger equation with $ \alpha = 1 $ using the weighted Jacobi method.}
    \label{fig:weighted_jacobi_1}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Weighted_Lax1000_01.pdf}
    \caption{Solution to the time-independent Schrodinger equation with $ \alpha = 1000 $ using the weighted Jacobi method.}
    \label{fig:weighted_jacobi_1000}
\end{figure*}

\subsection{Econo-physics}
\label{subsec:econo_physics}

In this section, we solve the Black-Scholes equation
\begin{align}
    \frac{\partial V}{\partial t} + \frac{1}{2} \sigma^2 S^2 \frac{\partial^2 V}{\partial S^2} - rV + r S \frac{\partial V}{\partial S} &= 0
\end{align}
with final stock value
\begin{align}
V(S, t=100) &= \begin{cases} 0 & \text{for } 0 \leq S < 10 \\ S-10 & \text{for } 10 \leq S \leq 20 \end{cases}
\end{align}
with $ \sigma = 0.1 $ and $ r = 0.05 $.

We use a forward difference for the time derivative (to keep the method explicit), a centred difference for the first derivative (for second order accuracy and stability) and a centred difference for the second derivative (standard choice).
The resulting finite difference equation is
\begin{align}
    V^{n+1}_j &= V^n_j + \frac{1}{2} \sigma^2 \left( S^n_j \right)^2 \frac{V^n_{j+1} - 2 V^n_j + V^n_{j-1}}{(\Delta S)^2} - \nonumber
    \\
    & \quad - r V^n_j + r S^n_j \frac{S^n_{j+1} - S^n_{j-1}}{2 \Delta S}
\end{align}

Using this finite difference method, the equation above was solved (with the boundary values held at their initial values), and the results are plotted in Figure~\ref{fig:stock}.
We can see the evolution of the stock over time.
As $ t $ decreases from 100, wee see a sort of ``bubble'' appear near $ S = 10 $.
$ V $ eventually evolves towards a simple straight line.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Stock.pdf}
    \caption{Numerical solution to the Black-Scholes equation.}
    \label{fig:stock}
\end{figure*}

\section{Solvers for the Riemann problem}
\label{sec:solvers_for_the_riemann_problem}

Fluid dynamics problems with discontinuities are quite difficult to deal with numerically.
This type of singular character cannot be dealt with by simple finite difference schemes, because these schemes are not TVD (total variation diminishing).
In a more physical sense, most schemes do not enforce conservation laws, which are fundamental in any physics problem.
By enforcing conservation laws through finite volume methods, we reach methods which are more complicated, but do a better job of dealing with discontinuities.

A common example of such a singular problem is the Sod shock tube problem.
Using a modified version of the code from Alex Tennant and Zachary Shand's presentation, we solved the Sod shock tube problem analytically using the parameters given in the lab manual.
The results are seen in Figure~\ref{fig:shocktube}.
It is important to note that the ``Energy'' plotted here is the \emph{total energy} $ E $ and not the specific internal energy $ \epsilon $.
Some solutions in other sources (including the code from Alex and Zach's presentation) plot $ \epsilon $ rather than $ E $.

In the analytical solution, we see the five key regions of the Sod shock tube solution.
We can see the boundaries of the regions as points where the solution is not smooth.
\begin{itemize}
\item
    Region 1 extends up to the first boundary, and it is where the fluid is unaffected from its initial condition.
\item
    Region 2 extend to the next boundary (just left of $ x = 0 $) and it represents the left-moving rare-fraction wave.
    This wave moves at the speed of sound of the undisturbed fluid.
\item
    Region 3 extends to the next boundary (right of $ x = 0 $).
    It represents the ``post shock'' which moves more slowly than the initial shock.
\item
    Region 4 extends to the next boundary and represents the initial shock which moved away from $ x = 0 $ with high speed.
\item
    The final region, Region 5, represents the region to the right which is so far unaffected by the shock.
\end{itemize}

Two methods for solving this problem are the Godunov method and the Van-Leer flux splitting method.
Both of these methods are flux-conservative methods, which have been studied above repeatedly.
The first key to these methods is that the studied quantities are replaced by their averages over the entire unit cell (making them finite volume methods).
The flux is then calculated at the edge of these volumes, ensuring that conservation conditions are met.

In principle, we could simply calculate the values of the flux using the simple formulas for them.
However, it is shown in practise that these methods do not work very well.
The Godunov method tries to fix this by using the analytical solution to the Riemann problem at every boundary of a unit cell.
This produces much better results than simple schemes because it gives a more accurate way of determining the flux at the boundary.

The Van-Leer scheme does two things: first of all it splits the flux so that the right-moving flux is calculated on the left boundary of the cell, and the left-moving flux is calculated on the right boundary of the cell.
The second thing it does is it limits the flux to help deal with strong discontinuities better by smoothing them out.
This may lead to mathematically less-accurate solutions, but the solution are more true to reality.

The key advantage of the Van-Leer scheme is that it does not require a root-finding method at each step.
Unfortunately, the solution to the Riemann problem is only semi-analytical in that it requires a numerical root-finding to get the full solution.
This slows down the Godunov method significantly.
Also, the Van Leer scheme is a Total Variation Diminishing scheme, which reduces the appearance of spurious oscillations near discontinuities.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/shocktube.pdf}
    \caption{Analytical solution to the Sod shocktube problem.}
    \label{fig:shocktube}
\end{figure}

We solved this problem using three different numerical methods: the Euler method, the Lax-Wendroff method, and the Van Leer flux splitting method, using a CFL condition of 0.5 in each case.
These results are seen in Figures~\ref{fig:euler_shock}, \ref{fig:lax_shock}, \ref{fig:flux_shock}.
In each case, the analytical solution is plotted for comparison.

We see that the Euler method is unstable, even though the CFL condition is met.
As we have seen before, the forward Euler method is almost always unsuitable.
We see that the Lax-Wendroff method is the most accurate of the three.
However, it introduces spurious oscillations near the discontinuities.
These oscillations are too big a price to pay for accuracy on the smoother portions.

The best method is the Van Leer flux-splitting method.
Though it is not quite as accurate as the Lax-Wendroff method (it smooths out the discontinuities) it does a much better job of preserving the fidelity of the solution.
For this reason, it provides a much better approach overall.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Euler.pdf}
    \caption{Solution to the Sod shocktube problem using the Euler method.}
    \label{fig:euler_shock}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Lax.pdf}
    \caption{Solution to the Sod shocktube problem using the Lax-Wendroff method.}
    \label{fig:lax_shock}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Flux.pdf}
    \caption{Solution to the Sod shocktube problem using the Van Leer flux-splitting method.}
    \label{fig:flux_shock}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}

In this report, we looked at a wide range of finite difference approaches for solving partial differential equations.
We saw that there is no one easy way to solve a partial differential equation.
Each equation and each situation must be carefully evaluated in order to pick a scheme which is fast, stable, easy to implement, and accurate.
This is an important and useful skill for any scientist or engineer, since partial differential equations show up in a huge range of places.
If these equations cannot be solved analytically, it is important to be able to pick an appropriate numerical scheme.

\onecolumn

\includepdf[pages={-}]{../Plots/Advection.pdf}

\section{Code}
\label{sec:code}

\subsection{Laplace equation code}
\label{subsec:laplace_equation_code}

\lstinputlisting[breaklines]{../Laplace.f90}
\vspace{10pt}

\subsection{Advection equation code}
\label{subsec:advection_equation_code}

\lstinputlisting[breaklines]{../Advection.f90}
\vspace{10pt}

\subsection{Diffusion equation code}
\label{subsec:diffusion_equation_code}

\lstinputlisting[breaklines]{../Diffusion.f90}
\vspace{10pt}

\subsection{Advection-diffusion equation code}
\label{subsec:advection_diffusion_equation_code}

\lstinputlisting[breaklines]{../AD_combined.f90}
\vspace{10pt}

\subsection{Burgers inviscid equation code}
\label{subsec:burgers_inviscid_equation_code}

\lstinputlisting[breaklines]{../Burgers_inviscid.f90}
\vspace{10pt}

\subsection{Burgers general equation code}
\label{subsec:burgers_general_equation_code}

\lstinputlisting[breaklines]{../Burgers_general.f90}
\vspace{10pt}

\subsection{Quantum mechanics code}
\label{subsec:quantum_mechanics_code}

\lstinputlisting[breaklines]{../Quantum.f90}
\vspace{10pt}

\subsection{Econo-physics code}
\label{subsec:econo_physics_code}

\lstinputlisting[breaklines]{../EconoPhys.f90}
\vspace{10pt}

\subsection{Riemann problem analytical code}
\label{subsec:riemann_problem_analytical_code}

\lstinputlisting[breaklines]{../Shocktest.f90}
\vspace{10pt}

\subsection{Riemann problem numerical code}
\label{subsec:riemann_problem_numerical_code}

\lstinputlisting[breaklines]{../Nobody_likes_CFD.f90}
\vspace{10pt}

\end{document}
