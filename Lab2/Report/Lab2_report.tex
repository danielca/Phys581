\documentclass[twocolumn]{myarticle}

\usepackage{mymacros}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{mathtools}

\lstset{%
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
stepnumber=1,}

\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\d}{\mathrm{d}}

\begin{document}

\title{Physics 581, Lab 2:\\Hooray for Fourier!}
\author{Casey Daniel and Chris Deimert}
\date{\today}

\maketitle

\section{Introduction}
\label{sec:introduction}

\section{Warm up}
\label{sec:warm_up}

\subsection{Using Numerical Recipes routines}
\label{subsec:using_numerical_recipes_routines}

In this section, we take the discrete Fourier transform (DFT) of four sampled signals using the fast Fourier transform (FFT) routine from Numerical Recipes.
The resulting power spectra (in dB) are shown in Figure~\ref{fig:warm_up}.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Warm_up.pdf}
    \caption{%
        The power spectra of the DFT's of four signals.
        Each signal was sampled at $ t = i/1024 $ for $ i = 1, 2, \ldots, 1024 $.
        The frequencies resulting from the discrete Fourier transform are then $ f = -511, -510, \ldots, 512 $.
    }
    \label{fig:warm_up}
\end{figure*}

In the top left of Figure~\ref{fig:warm_up}, we see the power spectrum of 
\[ 
    \sin(2 \pi \cdot 100 t) + 0.5 \sin (2 \pi \cdot 200 t) . 
\]
The power of the $ \SI{200}{Hz} $ signal is less than the power of the $ \SI{100}{Hz} $ signal, as it should be (the peak is about $ \SI{5}{dB} $ lower).
Also, we see that the spectrum is very sharply peaked, with around a $ \SI{150}{dB} $ power drop over a very short frequency range.

In the top right of Figure~\ref{fig:warm_up}, we use the same signal, but perturb one of the frequencies by $ \SI{0.5}{Hz} $ to get
\[
    \sin(2 \pi \cdot 100.5 t) + 0.5 \sin (2 \pi \cdot 200 t) . 
\]
Here we see a spreading of the power of the $ \SI{100.5}{Hz} $ signal.
This is because the frequencies of the DFT are integer multiples of $ \SI{1}{Hz} $.
In the last case, both sinusoids lined up perfectly with a DFT frequency, leading to very clean peaks. 
In this case, the $ \SI{100.5}{Hz} $ signal lies right in between two frequencies, leading to a spread in the spectrum when it should be very sharply peaked like a delta function.
So the DFT is not as good at capturing frequency components in between the native DFT frequencies.

In the bottom left of Figure~\ref{fig:warm_up}, we see an amplitude modulated signal
\[
    \left[ 2 + \sin(2\pi \cdot 8 t) \right] \cdot \sin ( 2 \pi \cdot 100 t)
\]
Here $ \SI{100}{Hz} $ is the carrier frequency and the message is $ 2 + \sin(2 \pi \cdot 8 t) $.
The spectrum of the message contains three peaks at $ \SI{-2}{Hz} $, $ \SI{0}{Hz} $, and $ \SI{2}{Hz} $.
By multiplying the message with a $ \SI{100}{Hz} $ carrier wave, we shift the message spectrum.
Instead of three peaks centred on $ \SI{0}{Hz} $, it is now three peaks centred on $ \SI{+-100}{Hz} $, with an additional boost to the $ \SI{+-100}{Hz} $ peak because of the carrier wave itself.
This can be seen in the figure, though it is difficult, because the bandwidth of the message is small compared to the carrier frequency.

In the bottom right of Figure~\ref{fig:warm_up}, we see a frequency modulated signal:
\[
    \sin \Big( 2 \pi \cdot 100 \big( 1 + 0.1 \sin (2 \pi \cdot 8 t) \big) t \Big)
\]
The carrier frequency is $ \SI{100}{Hz} $ and the message is proportional to $ t \cdot \sin(2 \pi \cdot 8 t) $.
This is a very wide-band message with a large amplitude, so we would expect the resulting frequency-modulated signal to have an extremely large bandwidth compared with the carrier.
This is why we do not see any significant peaks near $ \SI{100}{Hz} $ or any drop-off at higher frequencies.
Thanks to aliasing and the wide-band nature of the signal, we see a near-uniform mess of power across the frequency range of the DFT.

\subsection{Hearing yourself}
\label{subsec:hearing_yourself}

As another simple application of the FFT, the SOX software was used to record 10 seconds of speech.
An FFT was applied to the waveform, and the resulting power spectrum is plotted in Figure~\ref{fig:my_voice}.
We can see in the figure that most of the power lies with frequencies of about $ 10\,000 $ Hz or less, which is typical for audio.

An unusual feature here is the increase in power near $ 10\,000 $ Hz, which is a relatively high frequency for voice.
Typically, we would expect these high frequency components to carry less power than the low and medium frequency components.
This feature is likely due to the clipping of the signal, which can be seen in the time domain representation.
This was caused by the poor recording equipment of the laptop.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/My_voice.png}
    \caption{The time and frequency domain representations of 10 seconds of speech.}
    \label{fig:my_voice}
\end{figure*}


\section{Anti-Aliasing, Spectral Leakage, and the FFT}
\label{sec:anti_aliasing_spectral_leakage_and_the_fft}

\subsection{DFTs}
\label{subsec:dfts}

To make a DFT, we can simply take a finite number of periodic samples of a continuous signal. 
Let's take the following cosine with frequency $\omega$.
\begin{align}
    x(t)=\cos\left(\omega t\right)
\end{align}
If we take N samples, at an angular sampling frequency of $\omega_{S}$ we arrive with the following.
\begin{align}
    x[n]=\cos\left( \frac{2 \pi \omega n}{\omega_{s}}\right)
\end{align}
We are now ready for the DFT.
\begin{align}
    X_{k} &= \sum^{N-1}_{n=0}x[n]e^{-i 2\pi k n/N} 
    \\
    X_{k} &= \sum^{N-1}_{n=0}\cos\left(\frac{2 \pi \omega n}{\omega_{s}}\right) e^{-i 2\pi kn/N}
    \\
    X_{k} &= \sum^{N-1}_{n=0} \frac{1}{2} \left( e^{i 2 \pi \omega n / \omega_s} + e^{-i 2 \pi \omega n / \omega_s} \right) e^{-i 2\pi kn/N}
    \\
    X_{k} &= \sum^{N-1}_{n=0} \frac{1}{2} \left( e^{i 2 \pi n (\omega/\omega_s - k/N)} + e^{-i 2 \pi n (\omega / \omega_s + k/N)} \right)
\end{align}

%Now let $\omega_{s}=2\pi f_{s}$ be the angular sampling frequency. We can also let $\omega_{s} = \pm \omega$.
%\begin{align}
%    X_{K} &= \sum^{N-1}_{n=0}\cos\left(\frac{2\pi \omega n}{\omega_{s}}\right)e^{-i 2\pi kn/N}
%    \\
%    X_{k} &= \sum^{N-1}_{n=0}\cos\left(\pm 2\pi n\right)e^{-i 2\pi kn/N} 
%    \\
%    X_{k} &= \sum^{N-1}_{n=0}e^{\frac{-2\pi i kn}{N}}
%\end{align}
Noting that
\begin{align}
    \sum_{n=0}^{N-1}e^{\alpha n} = \frac{1-e^{\alpha N}}{1-e^{\alpha}}
\end{align}
(geometric series), we get
\begin{align}
    \Aboxed{X_{k} &= \frac{1}{2} \left( \frac{1 - e^{i 2 \pi (\omega N / \omega_s - k)}}{1 - e^{i 2 \pi (\omega / \omega_s - k/N)}} + \frac{1 - e^{-i 2 \pi (\omega N / \omega_s + k)}}{1 - e^{-i 2 \pi (\omega / \omega_s + k/N)}} \right)}
    %\\
    %\Aboxed{X_{k} &= \frac{1}{2} \left( \frac{1 - e^{i 2 \pi \omega N / \omega_s}}{1 - e^{i 2 \pi (\omega / \omega_s - k/N)}} + \frac{1 - e^{-i 2 \pi \omega N / \omega_s}}{1 - e^{-i 2 \pi (\omega / \omega_s + k/N)}} \right)}
\end{align}

At the point of interest, $ \omega_s = \omega N / k $, we get
\begin{align}
    X_{k} &= \frac{1}{2} \left( \frac{1 - e^{i 2 \pi (k - k)}}{1 - e^{i 2 \pi (k/N - k/N)}} + \frac{1 - e^{-i 2 \pi (k + k)}}{1 - e^{-i 2 \pi (k/N + k/N)}} \right)
\end{align}
The second term in the brackets is simply zero.
The first term is the indeterminate form $ 0/0 $, which must be evaluating using l'Hospital's rule.
Doing this, we see that the first sum is simply $ N $.
And so we end up with
\begin{align}
    X_k &= \frac{N}{2}
\end{align}
as expected.

The same result holds for $ \omega_s = -\omega N / k $.
This time, the first term in the brackets is zero and the second term is $ N $, again giving
\begin{align}
    X_k &= \frac{N}{2}
\end{align}

This is, in fact, the maximum possible amplitude of the DFT.
In general, the maximum amplitude occurs when $ k $ (an integer) is closest to $ \pm \omega N / \omega_s $.

We now look at a different function 
\begin{align}
    x[n] = \cos\left(2\pi f n\Delta t\right)
\end{align}
Using the attached source code, we were able to sample this function, and perform a DFT.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/AA1.pdf}
    \caption{%
        DFT of $x[n]=\cos(2\pi fn\Delta t)$. Top figure showing the sampled time series, the middle showing the DFT, and the bottom displaying the power in dB.
    }
    \label{fig:AA1}
\end{figure*}

The first thing to notice in figure \ref{fig:AA1} is the time series does not look like your typical cosine function. 
Our sampling frequency is close to that of our wave. 
This grabs a majority of the information, however the exact curves are lost. 

In the frequency domain, we see exactly what we expect: peaks at frequencies of $ \pm 0.25 $ Hz. 
Consistent with the results above, we see that the maximum amplitude in the frequency domain is $ N/2 = 32 $.

Since $ \omega/\omega_s $ is an integer here, the results above tell us that the DFT will be zero for all $ k $ other than the peak value, which is what we see.
In the power spectrum, we see that these values are not exactly zero, but they are at $ -120 $ dB, which is likely just due to numerical error.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/AA2.pdf}
    \caption{%
        DFT of the modified cosine of $x[n]=\cos(2\pi (f+\pi/8) n\Delta t)$. Top figure showing the sampled time series, the middle showing the DFT, and the bottom displaying the power in dB.
    }
    \label{fig:AA2}
\end{figure*}
Shifting the frequency to $f=f+\frac{\pi}{8}$ distorts the sampled time signal significantly, as shown in figure \ref{fig:AA2}. 
This is because the sampling frequency no longer aligns nicely with the signal frequency.
However, in the frequency domain, we can see that the signal is not distorted nearly as much.
We still have peaks of about $ N/2 = 32 $, but the other $ k $ values are no longer exactly zero.

This demonstrates the Shannon sampling theorem.
Even though the sampling rate is low enough that the time signal looks completely distorted, we can see from the frequency domain that we still have most of the information about the signal.

%This is because we have changed the frequency in such a way that we see the odd distortion. 
%We are still below our Nyquist frequency of 0.5Hz. 
%This is because we are sampling close to that of the waveform. 
%We can see in both the FFT and the power spectrum this has had. 
%The size of the peaks have been reduced, and the entire spectrum has a higher floor. 
%This is most noticeable in the power spectrum, where when compared to figure \ref{fig:AA1} the power is higher over all, and the peaks are less defined.   

\subsection{Gibbs Phenomenon}
\label{sec:gibbs_phenomenon}

Gibbs phenomenon is not restricted to Fourier series, but can also be found in DFTs. 
There is an implicit assumption in the DFT that the finite time series is repeated from negative infinity and to positive infinity. 
When the start and end points are the same, this is fine. 
However as we see in middle panel of figure \ref{fig:Window1} that a mismatch of the start and end does have a very large effect. 
The power spectrum below shows a very large increase in overall power and less-defined peaks. 
This is because the DFT is trying to compensate for the mismatch by adding in larger frequencies. 
Windowing is a way to lessen these effects. 
The Hanning window is a raised cosine that has a frequency of twice the period. 
Multiplying the window and the time series we can see it's effect in the third panel. 
The power spectrum shows better defined peaks, and a lower power between them. 
While not as defined as the first panel, the result is still better then with no window. 
Another commonly used windows is the Blackman Window, as shown in figure \ref{fig:Window2}. 
The overall result is the same, however the Blackman window has slightly better defined peaks, and less power in the sides of the peaks. 
Both these windows help when your data set does not have the same boundary points.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Window1.pdf}
    \caption{%
        Looking at the Hanning window. The top resents the time series of a plain sine wave, a sine wave that is not periodic in the interval, and the effect of the Hanning window on the non-periodic wave. The bottom displays the corresponding power spectra.
    }
    \label{fig:Window1}
\end{figure*}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Window2.pdf}
    \caption{%
        Looking at the Blackman window. The top resents the time series of a plain sine wave, a sine wave that is not periodic in the interval, and the effect of the Hanning window on the non-periodic wave. The bottom displays the corresponding power spectra.
    }
    \label{fig:Window2}
\end{figure*}

\section{Noise and the FFT}
\label{sec:noise_and_the_fft}

\subsection{Auto-correlation and power spectral density}
\label{subsec:auto_correlation_and_power_spectral_density}

For white noise, $ x[n] $ is an independent random variable for each $ n $ with zero mean and unit variance.
The autocorrelation is
\begin{align}
    AC[k] &= E \left( x[n] x[n-k] \right)
\end{align}
For $ k \neq 0 $, $ x[n] $ and $ x[n-k] $ are independent by definition, so $ AC[k \neq 0] = 0 $.
For $ k = 0 $,
\begin{align}
    AC[0] &= E \left( (x[n])^2 \right)
\end{align}
Since $ x[n] $ has zero mean, this is just the variance of $ x[n] $, which is 1.
Thus
\begin{align}
    AC[k] &= \delta[k]
\end{align}

The spectrum is then
\begin{align}
    S(\omega) &= \sum_{k = -\infty}^{\infty} AC[k] e^{-j \omega k}
    \\
    S(\omega) &= \sum_{k = -\infty}^{\infty} \delta[k] e^{-j \omega k}
    \\
    S(\omega) &= e^{-j \omega 0}
    \\
    S(\omega) &= 1
\end{align}

A white noise signal was generated using the gfortran PRN generator, shifted and scaled so that each step was a uniform random variable with zero mean and unit variance.
The autocorrelation function and the resulting spectrum are plotted in Figure~\ref{fig:white_noise_uniform}.
We can see that the autocorrelation function approximates $ \delta[k] $ and that the auto-correlation function is approximately unity (0 dB) across the frequencies.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/White_noise_uniform.pdf}
    \caption{Autocorrelation and power spectral density of uniform white noise.}
    \label{fig:white_noise_uniform}
\end{figure}

Another white noise signal was generated, but this time using Gaussian random variables of zero mean and unit variance.
The autocorrelation function and the resulting spectrum are plotted in Figure~\ref{fig:white_noise_gaussian}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/White_noise_gaussian.pdf}
    \caption{Autocorrelation and power spectral density of Gaussian white noise.}
    \label{fig:white_noise_gaussian}
\end{figure}

Comparing the two, we see that the Gaussian white noise has more power near zero frequency.
This is not surprising, since the Gaussian distribution is peaked near zero while the uniform distribution is not.

\subsection{Noise reduction}
\label{subsec:noise_reduction}

As an application of Fourier transforms, we add noise to a signal and then remove it in the frequency domain.
The process is seen in Figure~\ref{fig:noise_reduction}.
We can see the noise added in both domains.
In the frequency domain, we see the flat power spectral density of white noise added to the spectrum of the original signal.

Because the noise does not have too much power, we can still see dominant frequencies at 0.44 and 0.88 kHz.
These peaks correspond to the spectrum of the original signal, which has a 440 Hz and an 880 Hz sinusoid.
To completely distort the signal, we would need the spectral power of the white noise to overwhelm the power of the original signal, which is about 40 dB.
This means that the variance of $ r_i $ would need to be on the order of $ 100 $.
Thus, if $ r_i $ is a uniform random variable, it would need to have a range of about 10.

By zeroing out the frequency components which have low power, we end up with a cleaned signal which matches the original signal quite closely.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Noise_reduction.pdf}
    \caption{Use of Fourier transform to remove noise from a signal.}
    \label{fig:noise_reduction}
\end{figure*}

\subsection{Noise in signals with trends}
\label{subsec:noise_in_signals_with_trends}

We perform the same analysis as above, but this time the signal has a linear trend added.
In Figure~\ref{fig:signal_with_trend}, we can see that the noise removal technique we used in the last section does not work as well this time.
The cleaned up signal does not quite match the original signal.

However, if we remove the trend from the noisy signal, zero out the low-power frequencies, and then add back the trend, we get much better results.
This is seen in Figure~\ref{fig:signal_with_trend2}.

We learn from this that if a signal has a trend, it is important to know what that trend is ahead of time.
If the trend can't be removed before the noise reduction, it may be difficult or impossible to recover the original signal.
This suggests a possible encryption technique: adding a trend to a signal makes it very difficult to decrypt the signal unless the trend is known.
Thus, the trend acts as a kind of ``key'' for the signal, so that only people who know what the trend is are able to recover the original signal.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Signal_with_trend.pdf}
    \caption{Noise removed from a signal with a trend.}
    \label{fig:signal_with_trend}
\end{figure*}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Signal_with_trend2.pdf}
    \caption{Noise removed from a signal with a trend. This time the trend was removed before the noise was removed, and then added back in later.}
    \label{fig:signal_with_trend2}
\end{figure*}

\subsection{Application to financial series}
\label{subsec:application_to_financial_series}

As another example of Fourier analysis on noisy data, we look at real world stock prices.
In Figure~\ref{fig:stocks}, we see the series of stocks for each company.
We also see the continuously compounded returns, the auto-correlation function and the power spectrum in the frequency domain.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Stocks.pdf}
    \caption{Analysis of real world stock prices.}
    \label{fig:stocks}
\end{figure*}

From the auto-correlations, we can see that these signals are strongly correlated except at certain points where the auto-correlation drops off.
This indicates that stock prices are not completely random, however they do undergo strong unpredictably random changes.
The high autocorrelations might lead someone to believe that they can predict where a stock is going, but the sudden drops indicate that these predictions my fail drastically.

From the spectrum, we see that most of the power is concentrated near DC and low-frequency components.
However, there are still noticeable higher-frequency components.
Since the spectrum is not flat, this does not look like white noise superimposed on an underlying signal.
Rather, the spectrum looks like noise with a stronger low-frequency component.

\onecolumn

\section{Code}
\label{sec:code}

\subsection{Fourier analysis module}
\label{subsec:fourier_analysis_module}

\lstinputlisting[breaklines]{../../Modules/Fourier_analysis_module.f90}
\vspace{10pt}

\subsection{Warm up main code}
\label{subsec:warm_up_main_code}

\lstinputlisting[breaklines]{../Warm_up.f90}
\vspace{10pt}

\end{document}
