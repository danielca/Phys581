\documentclass[twocolumn]{myarticle}

\usepackage{mymacros}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{mathtools, cuted}

\lstset{%
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
stepnumber=1,}

\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\sinc}{\text{sinc}}
\renewcommand{\d}{\mathrm{d}}

\begin{document}

\title{Physics 581, Lab 2:\\Hooray for Fourier!}
\author{Casey Daniel and Chris Deimert}
\date{\today}

\maketitle

\section{Introduction}
\label{sec:introduction}

In this lab we explore the application of the Fourier transform in numerical methods.
In particular, we look at the discretized version: the discrete Fourier transform (DFT).
For most of the analysis we use the fast Fourier transform (FFT), as implemented in Numerical Recipes.
The FFT is simply an algorithm for calculating DFT as quickly as possible.

The Fourier transform has a huge range of applicability, especially in physics and electrical engineering applications.
We will see why in this lab, as Fourier analysis proves itself useful again and again in vastly different contexts.

\section{Warm up}
\label{sec:warm_up}

(The code for this section can be seen in Sections \ref{subsec:fourier_analysis_module}, \ref{subsec:warm_up_main_code}, and \ref{subsec:warm_up_plotting_code})

\subsection{Using Numerical Recipes routines}
\label{subsec:using_numerical_recipes_routines}

In this section, we take the discrete Fourier transform (DFT) of four sampled signals using the fast Fourier transform (FFT) routine from Numerical Recipes.
The resulting power spectra (in dB) are shown in Figure~\ref{fig:warm_up}.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Warm_up.pdf}
    \caption{%
        The power spectra of the DFT's of four signals.
        Each signal was sampled at $ t = i/1024 $ for $ i = 1, 2, \ldots, 1024 $.
        The frequencies resulting from the discrete Fourier transform are then $ f = -511, -510, \ldots, 512 $.
    }
    \label{fig:warm_up}
\end{figure*}

In the top left of Figure~\ref{fig:warm_up}, we see the power spectrum of 
\[ 
    \sin(2 \pi \cdot 100 t) + 0.5 \sin (2 \pi \cdot 200 t) . 
\]
The power of the $ \SI{200}{Hz} $ signal is less than the power of the $ \SI{100}{Hz} $ signal, as it should be (the peak is about $ \SI{5}{dB} $ lower).
Also, we see that the spectrum is very sharply peaked, with around a $ \SI{150}{dB} $ power drop over a very short frequency range.

In the top right of Figure~\ref{fig:warm_up}, we use the same signal, but perturb one of the frequencies by $ \SI{0.5}{Hz} $ to get
\[
    \sin(2 \pi \cdot 100.5 t) + 0.5 \sin (2 \pi \cdot 200 t) . 
\]
Here we see a spreading of the power of the $ \SI{100.5}{Hz} $ signal.
This is because the frequencies of the DFT are integer multiples of $ \SI{1}{Hz} $.
In the last case, both sinusoids lined up perfectly with a DFT frequency, leading to very clean peaks. 
In this case, the $ \SI{100.5}{Hz} $ signal lies right in between two frequencies, leading to a spread in the spectrum when it should be very sharply peaked like a delta function.
So the DFT is not as good at capturing frequency components in between the native DFT frequencies.

In the bottom left of Figure~\ref{fig:warm_up}, we see an amplitude modulated signal
\[
    \left[ 2 + \sin(2\pi \cdot 8 t) \right] \cdot \sin ( 2 \pi \cdot 100 t)
\]
Here $ \SI{100}{Hz} $ is the carrier frequency and the message is $ 2 + \sin(2 \pi \cdot 8 t) $.
The spectrum of the message contains three peaks at $ \SI{-2}{Hz} $, $ \SI{0}{Hz} $, and $ \SI{2}{Hz} $.
By multiplying the message with a $ \SI{100}{Hz} $ carrier wave, we shift the message spectrum.
Instead of three peaks centred on $ \SI{0}{Hz} $, it is now three peaks centred on $ \SI{+-100}{Hz} $, with an additional boost to the $ \SI{+-100}{Hz} $ peak because of the carrier wave itself.
This can be seen in the figure, though it is difficult, because the bandwidth of the message is small compared to the carrier frequency.

In the bottom right of Figure~\ref{fig:warm_up}, we see a frequency modulated signal:
\[
    \sin \Big( 2 \pi \cdot 100 \big( 1 + 0.1 \sin (2 \pi \cdot 8 t) \big) t \Big)
\]
The carrier frequency is $ \SI{100}{Hz} $ and the message is proportional to $ t \cdot \sin(2 \pi \cdot 8 t) $.
This is a very wide-band message with a large amplitude, so we would expect the resulting frequency-modulated signal to have an extremely large bandwidth compared with the carrier.
This is why we do not see any significant peaks near $ \SI{100}{Hz} $ or any drop-off at higher frequencies.
Thanks to aliasing and the wide-band nature of the signal, we see a near-uniform mess of power across the frequency range of the DFT.

\subsection{Hearing yourself}
\label{subsec:hearing_yourself}

As another simple application of the FFT, the SOX software was used to record 10 seconds of speech.
An FFT was applied to the waveform, and the resulting power spectrum is plotted in Figure~\ref{fig:my_voice}.
We can see in the figure that most of the power lies with frequencies of about $ 10\,000 $ Hz or less, which is typical for audio.

An unusual feature here is the increase in power near $ 10\,000 $ Hz, which is a relatively high frequency for voice.
Typically, we would expect these high frequency components to carry less power than the low and medium frequency components.
This feature is likely due to the clipping of the signal, which can be seen in the time domain representation.
This was caused by the poor recording equipment of the laptop.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/My_voice.png}
    \caption{The time and frequency domain representations of 10 seconds of speech.}
    \label{fig:my_voice}
\end{figure*}


\section{Anti-Aliasing, Spectral Leakage, and the FFT}
\label{sec:anti_aliasing_spectral_leakage_and_the_fft}

(The code for this section can be seen in Sections \ref{subsec:fourier_analysis_module}, \ref{subsec:aliasing_spectral_leakage_and_the_fft_code}, and \ref{subsec:aliasing_spectral_leakage_and_the_fft_plotting_code})

\subsection{DFTs}
\label{subsec:dfts}

To make a DFT, we can simply take a finite number of periodic samples of a continuous signal. 
Let's take the following cosine with frequency $\omega$.
\begin{align}
    x(t)=\cos\left(\omega t\right)
\end{align}
If we take N samples, at an angular sampling frequency of $\omega_{S}$ we arrive with the following.
\begin{align}
    x[n]=\cos\left( \frac{2 \pi \omega n}{\omega_{s}}\right)
\end{align}
We are now ready for the DFT.
\begin{align}
    X_{k} &= \sum^{N-1}_{n=0}x[n]e^{-i 2\pi k n/N} 
    \\
    X_{k} &= \sum^{N-1}_{n=0}\cos\left(\frac{2 \pi \omega n}{\omega_{s}}\right) e^{-i 2\pi kn/N}
    \\
    X_{k} &= \sum^{N-1}_{n=0} \frac{1}{2} \left( e^{i 2 \pi \omega n / \omega_s} + e^{-i 2 \pi \omega n / \omega_s} \right) e^{-i 2\pi kn/N}
    \\
    X_{k} &= \sum^{N-1}_{n=0} \frac{1}{2} \left( e^{i 2 \pi n (\omega/\omega_s - k/N)} + e^{-i 2 \pi n (\omega / \omega_s + k/N)} \right)
\end{align}

%Now let $\omega_{s}=2\pi f_{s}$ be the angular sampling frequency. We can also let $\omega_{s} = \pm \omega$.
%\begin{align}
%    X_{K} &= \sum^{N-1}_{n=0}\cos\left(\frac{2\pi \omega n}{\omega_{s}}\right)e^{-i 2\pi kn/N}
%    \\
%    X_{k} &= \sum^{N-1}_{n=0}\cos\left(\pm 2\pi n\right)e^{-i 2\pi kn/N} 
%    \\
%    X_{k} &= \sum^{N-1}_{n=0}e^{\frac{-2\pi i kn}{N}}
%\end{align}
Noting that
\begin{align}
    \sum_{n=0}^{N-1}e^{\alpha n} = \frac{1-e^{\alpha N}}{1-e^{\alpha}}
\end{align}
(geometric series), we get
\begin{align}
    \Aboxed{X_{k} &= \frac{1}{2} \left( \frac{1 - e^{i 2 \pi (\omega N / \omega_s - k)}}{1 - e^{i 2 \pi (\omega / \omega_s - k/N)}} + \frac{1 - e^{-i 2 \pi (\omega N / \omega_s + k)}}{1 - e^{-i 2 \pi (\omega / \omega_s + k/N)}} \right)}
    %\\
    %\Aboxed{X_{k} &= \frac{1}{2} \left( \frac{1 - e^{i 2 \pi \omega N / \omega_s}}{1 - e^{i 2 \pi (\omega / \omega_s - k/N)}} + \frac{1 - e^{-i 2 \pi \omega N / \omega_s}}{1 - e^{-i 2 \pi (\omega / \omega_s + k/N)}} \right)}
\end{align}

At the point of interest, $ \omega_s = \omega N / k $, we get
\begin{align}
    X_{k} &= \frac{1}{2} \left( \frac{1 - e^{i 2 \pi (k - k)}}{1 - e^{i 2 \pi (k/N - k/N)}} + \frac{1 - e^{-i 2 \pi (k + k)}}{1 - e^{-i 2 \pi (k/N + k/N)}} \right)
\end{align}
The second term in the brackets is simply zero.
The first term is the indeterminate form $ 0/0 $, which must be evaluating using l'Hospital's rule.
Doing this, we see that the first sum is simply $ N $.
And so we end up with
\begin{align}
    X_k &= \frac{N}{2}
\end{align}
as expected.

The same result holds for $ \omega_s = -\omega N / k $.
This time, the first term in the brackets is zero and the second term is $ N $, again giving
\begin{align}
    X_k &= \frac{N}{2}
\end{align}

This is, in fact, the maximum possible amplitude of the DFT.
In general, the maximum amplitude occurs when $ k $ (an integer) is closest to $ \pm \omega N / \omega_s $.

We now look at a different function 
\begin{align}
    x[n] = \cos\left(2\pi f n\Delta t\right)
\end{align}
Using the attached source code, we were able to sample this function, and perform a DFT.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/AA1.pdf}
    \caption{%
        DFT of $x[n]=\cos(2\pi fn\Delta t)$. Top figure showing the sampled time series, the middle showing the DFT, and the bottom displaying the power in dB.
    }
    \label{fig:AA1}
\end{figure*}

The first thing to notice in figure \ref{fig:AA1} is the time series does not look like your typical cosine function. 
Our sampling frequency is close to that of our wave. 
This grabs a majority of the information, however the exact curves are lost. 

In the frequency domain, we see exactly what we expect: peaks at frequencies of $ \pm 0.25 $ Hz. 
Consistent with the results above, we see that the maximum amplitude in the frequency domain is $ N/2 = 32 $.

Since $ \omega/\omega_s $ is an integer here, the results above tell us that the DFT will be zero for all $ k $ other than the peak value, which is what we see.
In the power spectrum, we see that these values are not exactly zero, but they are at $ -120 $ dB, which is likely just due to numerical error.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/AA2.pdf}
    \caption{%
        DFT of the modified cosine of $x[n]=\cos(2\pi (f+\pi/8) n\Delta t)$. Top figure showing the sampled time series, the middle showing the DFT, and the bottom displaying the power in dB.
    }
    \label{fig:AA2}
\end{figure*}
Shifting the frequency to $f=f+\frac{\pi}{8}$ distorts the sampled time signal significantly, as shown in figure \ref{fig:AA2}. 
This is because the sampling frequency no longer aligns nicely with the signal frequency.
However, in the frequency domain, we can see that the signal is not distorted nearly as much.
We still have peaks of about $ N/2 = 32 $, but the other $ k $ values are no longer exactly zero.

This demonstrates the Shannon sampling theorem.
Even though the sampling rate is low enough that the time signal looks completely distorted, we can see from the frequency domain that we still have most of the information about the signal.

%This is because we have changed the frequency in such a way that we see the odd distortion. 
%We are still below our Nyquist frequency of 0.5Hz. 
%This is because we are sampling close to that of the waveform. 
%We can see in both the FFT and the power spectrum this has had. 
%The size of the peaks have been reduced, and the entire spectrum has a higher floor. 
%This is most noticeable in the power spectrum, where when compared to figure \ref{fig:AA1} the power is higher over all, and the peaks are less defined.   

\subsection{Gibbs Phenomenon}
\label{sec:gibbs_phenomenon}

Gibbs phenomenon is not restricted to Fourier series, but can also be found in DFTs. 
There is an implicit assumption in the DFT that the finite time series is repeated from negative infinity and to positive infinity. 
When the start and end points are the same, this is fine. 
However as we see in middle panel of figure \ref{fig:Window1} that a mismatch of the start and end does have a very large effect. 
The power spectrum below shows a very large increase in overall power and less-defined peaks. 
This is because the DFT is trying to compensate for the mismatch by adding in larger frequencies. 
Windowing is a way to lessen these effects. 
The Hanning window is a raised cosine that has a frequency of twice the period. 
Multiplying the window and the time series we can see it's effect in the third panel. 
The power spectrum shows better defined peaks, and a lower power between them. 
While not as defined as the first panel, the result is still better then with no window. 
Another commonly used windows is the Blackman Window, as shown in figure \ref{fig:Window2}. 
The overall result is the same, however the Blackman window has slightly better defined peaks, and less power in the sides of the peaks. 
Both these windows help when your data set does not have the same boundary points.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Window1.pdf}
    \caption{%
        Looking at the Hanning window. The top resents the time series of a plain sine wave, a sine wave that is not periodic in the interval, and the effect of the Hanning window on the non-periodic wave. The bottom displays the corresponding power spectra.
    }
    \label{fig:Window1}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Window2.pdf}
    \caption{%
        Looking at the Blackman window. The top resents the time series of a plain sine wave, a sine wave that is not periodic in the interval, and the effect of the Hanning window on the non-periodic wave. The bottom displays the corresponding power spectra.
    }
    \label{fig:Window2}
\end{figure*}

\section{Noise and the FFT}
\label{sec:noise_and_the_fft}

(The code for this section can be seen in Sections \ref{subsec:fourier_analysis_module}, \ref{subsec:noise_and_the_fft_code}, \ref{subsec:random_numbers_module}, \ref{subsec:dr_ouyeds_random_numbers_module}, and \ref{subsec:noise_and_the_fft_plotting_code})

\subsection{Auto-correlation and power spectral density}
\label{subsec:auto_correlation_and_power_spectral_density}

For white noise, $ x[n] $ is an independent random variable for each $ n $ with zero mean and unit variance.
The autocorrelation is
\begin{align}
    AC[k] &= E \left( x[n] x[n-k] \right)
\end{align}
For $ k \neq 0 $, $ x[n] $ and $ x[n-k] $ are independent by definition, so $ AC[k \neq 0] = 0 $.
For $ k = 0 $,
\begin{align}
    AC[0] &= E \left( (x[n])^2 \right)
\end{align}
Since $ x[n] $ has zero mean, this is just the variance of $ x[n] $, which is 1.
Thus
\begin{align}
    AC[k] &= \delta[k]
\end{align}

The spectrum is then
\begin{align}
    S(\omega) &= \sum_{k = -\infty}^{\infty} AC[k] e^{-j \omega k}
    \\
    S(\omega) &= \sum_{k = -\infty}^{\infty} \delta[k] e^{-j \omega k}
    \\
    S(\omega) &= e^{-j \omega 0}
    \\
    S(\omega) &= 1
\end{align}

A white noise signal was generated using the gfortran PRN generator, shifted and scaled so that each step was a uniform random variable with zero mean and unit variance.
The autocorrelation function and the resulting spectrum are plotted in Figure~\ref{fig:white_noise_uniform}.
We can see that the autocorrelation function approximates $ \delta[k] $ and that the auto-correlation function is approximately unity (0 dB) across the frequencies.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/White_noise_uniform.pdf}
    \caption{Autocorrelation and power spectral density of uniform white noise.}
    \label{fig:white_noise_uniform}
\end{figure}

Another white noise signal was generated, but this time using Gaussian random variables of zero mean and unit variance.
The autocorrelation function and the resulting spectrum are plotted in Figure~\ref{fig:white_noise_gaussian}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/White_noise_gaussian.pdf}
    \caption{Autocorrelation and power spectral density of Gaussian white noise.}
    \label{fig:white_noise_gaussian}
\end{figure}

Comparing the two, we see that the Gaussian white noise has more power near zero frequency.
This is not surprising, since the Gaussian distribution is peaked near zero while the uniform distribution is not.

\subsection{Noise reduction}
\label{subsec:noise_reduction}

As an application of Fourier transforms, we add noise to a signal and then remove it in the frequency domain.
The process is seen in Figure~\ref{fig:noise_reduction}.
We can see the noise added in both domains.
In the frequency domain, we see the flat power spectral density of white noise added to the spectrum of the original signal.

Because the noise does not have too much power, we can still see dominant frequencies at 0.44 and 0.88 kHz.
These peaks correspond to the spectrum of the original signal, which has a 440 Hz and an 880 Hz sinusoid.
To completely distort the signal, we would need the spectral power of the white noise to overwhelm the power of the original signal, which is about 40 dB.
This means that the variance of $ r_i $ would need to be on the order of $ 100 $.
Thus, if $ r_i $ is a uniform random variable, it would need to have a range of about 10.

By zeroing out the frequency components which have low power, we end up with a cleaned signal which matches the original signal quite closely.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Noise_reduction.pdf}
    \caption{Use of Fourier transform to remove noise from a signal.}
    \label{fig:noise_reduction}
\end{figure*}

\subsection{Noise in signals with trends}
\label{subsec:noise_in_signals_with_trends}

We perform the same analysis as above, but this time the signal has a linear trend added.
In Figure~\ref{fig:signal_with_trend}, we can see that the noise removal technique we used in the last section does not work as well this time.
The cleaned up signal does not quite match the original signal.

However, if we remove the trend from the noisy signal, zero out the low-power frequencies, and then add back the trend, we get much better results.
This is seen in Figure~\ref{fig:signal_with_trend2}.

We learn from this that if a signal has a trend, it is important to know what that trend is ahead of time.
If the trend can't be removed before the noise reduction, it may be difficult or impossible to recover the original signal.
This suggests a possible encryption technique: adding a trend to a signal makes it very difficult to decrypt the signal unless the trend is known.
Thus, the trend acts as a kind of ``key'' for the signal, so that only people who know what the trend is are able to recover the original signal.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Signal_with_trend.pdf}
    \caption{Noise removed from a signal with a trend.}
    \label{fig:signal_with_trend}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Signal_with_trend2.pdf}
    \caption{Noise removed from a signal with a trend. This time the trend was removed before the noise was removed, and then added back in later.}
    \label{fig:signal_with_trend2}
\end{figure*}

\subsection{Application to financial series}
\label{subsec:application_to_financial_series}

As another example of Fourier analysis on noisy data, we look at real world stock prices.
In Figure~\ref{fig:stocks}, we see the series of stocks for each company.
We also see the continuously compounded returns, the auto-correlation function and the power spectrum in the frequency domain.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Stocks.pdf}
    \caption{Analysis of real world stock prices.}
    \label{fig:stocks}
\end{figure*}

From the auto-correlations, we can see that these signals are strongly correlated except at certain points where the auto-correlation drops off.
This indicates that stock prices are not completely random, however they do undergo strong unpredictably random changes.
The high autocorrelations might lead someone to believe that they can predict where a stock is going, but the sudden drops indicate that these predictions my fail drastically.

From the spectrum, we see that most of the power is concentrated near DC and low-frequency components.
However, there are still noticeable higher-frequency components.
Since the spectrum is not flat, this does not look like white noise superimposed on an underlying signal.
Rather, the spectrum looks like noise with a stronger low-frequency component.

\section{Lorenz System}

(The code for this section can be seen in Sections \ref{subsec:fourier_analysis_module}, \ref{subsec:lorenz_system_code}, \ref{subsec:ode_system_module}, \ref{subsec:differential_equations_module}, \ref{subsec:lorenz_system_plotting_code})

\subsection{Fixed Points}
The Lorenz System, as studied in Phys 481, can be shown to have stable points, if certain conditions are met. Below the radius $r=1$, the only stable points is the trivial solution of (0,0,0). Between $r=r_{B}$ and $r=1$ the stable points are ($\pm \sqrtsign{b(r-1)}$,$\pm \sqrtsign{b(r-1)}$, $r-1$), and is otherwise unstable, where $r_{B}$ is defined by
\begin{equation*}
r_{B} = \frac{\sigma (\sigma + b + 3)}{\sigma - b -1} \approx 24.7
\end{equation*}
These stable points are shown in figure \ref{fig:fixed}. 
To generate this diagram, the system was run with a number of different initial conditions and $ r $ values, and the final $ x $ value was recorded and plotted as a function of $ r $.

We see the bifurcation of the stable points: the stable value is $ x = 0 $ until $ r =1 $ when they begin to diverge. 
Around $r=r_{B}$ we start to reach an unstable system. 
The mass arrangement of end points shows this chaos. 

This type of system can model convections. 
Given certain conditions, the trivial solution represents no mixing of fluids, while the bifurcation shows a smooth mixing point between the two. 
The unstable state is very quick and unsteady mixing.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/FixedPoints.pdf}
    \caption{Bifurcation Diagram for the Lorenz System.}
    \label{fig:fixed}
\end{figure*}

\subsection{Fourier Analysis}
Figures \ref{fig:Phase1} through \ref{fig:Phase26} show the system at different radii. 
Through these we can see how stable the system is. 
We see how quickly the solutions converge to the stable points. 
The Phase plot shows how much "mixing" is done before reaching the stable state. 
The interesting data is in the DFT. 
We can see in figure \ref{fig:Phase1} a very strong pulse at 0Hz, and a smooth curve down to the base. 
When we compare this to figure \ref{fig:Phase145}, we see that the curve down to the base has oscillations. 
This indicates that the system is more unstable. 
We see the curve is full of oscillations, and is wider. 
This indicates the instability in the system. From the DFT we are able to infer how stable the Lorenz System is.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Phase1.pdf}
    \caption{Time series, phase space, and DFT of the Lorenz System for R=1}
    \label{fig:Phase1}
\end{figure*}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Phase3.pdf}
    \caption{Time series, phase space, and DFT of the Lorenz System for R=3}
    \label{fig:Phase3}
\end{figure*}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Phase13.pdf}
    \caption{Time series, phase space, and DFT of the Lorenz System for R=13}
    \label{fig:Phase13}
\end{figure*}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Phase145.pdf}
    \caption{Time series, phase space, and DFT of the Lorenz System for R=14.5}
    \label{fig:Phase145}
\end{figure*}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Phase26.pdf}
    \caption{Time series, phase space, and DFT of the Lorenz System for R=26}
    \label{fig:Phase26}
\end{figure*}


Another way to dynamic nature of the Lorenz System is through a spectrogram. 
As in figure \ref{fig:Spectral} we see the radius along the horizontal axis, the frequency along the vertical axis, and the power in dB. 
We can see how much the system spirals around the stable points. 
For $ r < 1 $, the spectrum is strongly concentrated at DC, consistent with the fact that the system does not oscillate.
For $ 1 < r < r_B $, we can see peaks of increasing intensity in higher frequencies, indicating the growing oscillating character of the system with growing $ r $.
As $ r $ moves past the unstable point $ r_B $, we see the system move away from sharp spectral peaks and towards a smooth spectrum.
This is consistent with chaotic motion (noise) as opposed to oscillation (where we would see peaks in the spectrum).

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Spectral.pdf}
    \caption{Spectrogram showing the DFT of x(t). The Power in dB is displayed in the colour, the frequency along the y-axis and the radius along the x-axis}
    \label{fig:Spectral}
\end{figure*}

In general, Fourier analysis can provide information in a much ``cleaner'' fashion than time domain information.
To understand how this system's behaviour changes with $ r $ in the time domain, we would need to generate a number of time domain plots and compare them.
Seeing a smooth transition of system behaviour as a function of $ r $ would be very difficult with time domain information: we would need to generate some sort of animation showing the change of trajectories with $ r $.

In the frequency domain however, we can very easily see the behaviour as a function of $ r $ using a spectrograph like the one just discussed.
This spectrograph allows us to see the gradual changes in the system behaviour without too much effort.
Also, from the frequency domain, it is simple to see whether the system behaviour is resonant or not, and we can see exactly where those resonances are if they exist.
In the time domain, it may be difficult to distinguish systems with multiple resonances from noisy systems.

\subsection{Astrophysical Application}
The DFT can also be used on pulsar stars. 
Given a counts per given time period we can star to see if there any periodic tones to the signal pattern. 
In figure~\ref{fig:Astro} we see the time series. 
There is simply to much data with to much variation to see any kind of periodic nature of the signal. 
The signal looks pretty much like random noise.

Utilizing an FFT, as shown in the lower half of figure \ref{fig:Astro} plotted in dB, we can see specific frequencies rise above the rest. 
From these large peaks it is very apparent that there are strong periodic components in the pulsar's emissions, spaced a little less than 1 Hz apart.

We can see the effects of aliasing near the higher frequencies, where it looks like some higher frequency components have ``folded back'' into the lower frequency regime.
This indicates that we should increase the sampling rate in order to get a more accurate picture of the pulsar spectrum.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{../Plots/Astro.pdf}
    \caption{The time series and the DFT of the pulsar star signal}
    \label{fig:Astro}
\end{figure*}

\section{Image analysis}
\label{sec:image_analysis}

(The code for this section can be seen in Sections \ref{subsec:fourier_analysis_module}, \ref{subsec:image_analysis_code})

\subsection{Simple example}
\label{subsec:simple_example}

Here we take the 2D DFT of a simple matrix:
\begin{align}
    X &= \begin{bmatrix} 1 & 2 & 3 & 9 \\ 8 & 5 & 1 & 2 \\ 9 & 8 & 7 & 2 \end{bmatrix}
\end{align}

First we take the 1D DFT along each column to obtain the partially transformed matrix $ X_1 $.
Next we take the 1D DFT of $ X_1^T $, and then take the transpose again to obtain $ X_2 $, which is the 2D DFT of $ X $.
These results are shown below, broken up into real and imaginary parts.

\begin{align}
    \text{Re}[X_1] &= 
    \begin{bmatrix}
         15.0000  &
         -2.0000  &
         -7.0000  &
         -2.0000 
         \\
         16.0000  &
          7.0000  &
          2.0000  &
          7.0000 
         \\
         26.0000  &
          2.0000  &
          6.0000  &
          2.0000 
    \end{bmatrix}
    \\
    \text{Im}[X_1] &= 
    \begin{bmatrix}
          0.0000 &
          7.0000 &
          0.0000 &
         - 7.0000
        \\ 
          0.0000 &
         - 3.0000 &
          0.0000 &
          3.0000
        \\ 
          0.0000 &
         - 6.0000 &
         - 0.0000 &
          6.0000
    \end{bmatrix}
    \\
    \text{Re}[X_2] &=
    \begin{bmatrix}
         57.0000 &
          7.0000 &
          1.0000 &
          7.0000
         \\
         -6.0000 &
         -3.9019 &
        -11.0000 &
         -9.0981
         \\
         -6.0000 &
         -9.0981 &
        -11.0000 &
         -3.9019
    \end{bmatrix}
    \\
    \text{Im}[X_2] &=
    \begin{bmatrix}
          0.0000 &
        -  2.0000 &
          0.0000 &
          2.0000
        \\
          8.6603 &
          7.1699 &
          3.4641 &
        - 15.8301
        \\
        -  8.6603 &
         15.8301 &
        -  3.4641 &
        -  7.1699
    \end{bmatrix}
\end{align}

These results are exactly the same as the results predicted by Mathematica and Matlab.

\subsection{Convolution, noisy image, and filtering}
\label{subsec:convolution_noisy_image_and_filtering}

First, as an example of convolution, we convolve $ f(t) = t $ and $ h(t) = \sin t $.
\begin{align}
    (f * h)(t) &= \int_{0}^{t} f(t - \tau) h(\tau) d \tau
    \\
    (f * h)(t) &= \int_{0}^{t} (t - \tau) \sin(\tau) d \tau
    \\
    (f * h)(t) &= t \int_{0}^{t} \sin(\tau) d \tau - \int_{0}^{t} \tau \sin (\tau) d \tau
    \\
    (f * h)(t) &= t \left[ -\cos(t) + \cos(0) \right] - 
    \nonumber \\
    &\quad - \left[ \sin(t) - t \cos(t) - \sin(0) + 0 \cdot \cos(0) \right]
    \\
    (f * h)(t) &= - t \cos(t) + t - \sin(t) + t \cos(t)
    \\
    (f * h)(t) &= t - \sin(t)
\end{align}

As an example of noise reduction in an image, we take $ F(u,v) = 10^{-5} $, $ H(u,v) = 10^{-3} $, and $ N(u,v) = 10^{-6} $.
The distorted image at $ (u,v) $ is then
\begin{align}
    G(u,v) &= F(u,v) H(u,v) + N(u,v)
    \\
    G(u,v) &= 10^{-5} 10^{-3} + 10^{-6}
    \\
    G(u,v) &= 1.01 \times 10^{-6}
\end{align}
Now, if we try to calculate the original signal using $ F = G/H $, we get
\begin{align}
    \tilde{F}(u,v) &= \frac{1.01 \times 10^{-6}}{10^{-3}}
    \\
    \tilde{F}(u,v) &= 1.01 \times 10^{-3}
\end{align}

So even though the signal is larger than the noise, small noise $ N $ in combination with small $ H $ gives a completely wrong estimate of $ F $.

\subsection{Commonly used software}
\label{subsec:commonly_used_software}

By looking at the options menus of the popular image manipulating tool \emph{ImageMagick}, we can see some of the image processing techniques we've learned.
The Reduce Noise and Add Noise functions are likely subtractive and additive filters.
They allow us to pick the noise radius, which allows us to specify the nature of $ N(u,v) $ (whether it is Gaussian, white, etc.).
The sharpen and blur features likely apply multiplicative filters $ H(u,v) $.
The radius and standard deviation options let us specify what the impulse response function $ h(x,y) $ looks like.

\section{Conclusion}
\label{sec:conclusion}

We have seen again and again how useful the Fourier transform is in a wide range of context.
In particular, the FFT is an indispensable tool for any engineer or physicist, as it allows for quick transitions between the time and frequency domains.
Once in the frequency domain, characteristics of a system which may have been muddled in the time domain can be come immediately apparent.

\onecolumn

\section{Code}
\label{sec:code}

\subsection{Fourier analysis module}
\label{subsec:fourier_analysis_module}

\lstinputlisting[breaklines]{../../Modules/Fourier_analysis_module.f90}
\vspace{10pt}

\subsection{Warm up main code}
\label{subsec:warm_up_main_code}

\lstinputlisting[breaklines]{../Warm_up.f90}
\vspace{10pt}

\subsection{Warm up plotting code}
\label{subsec:warm_up_plotting_code}

\lstinputlisting[breaklines]{../Warm_up.gp}
\vspace{10pt}

\subsection{Aliasing, spectral leakage and the FFT code}
\label{subsec:aliasing_spectral_leakage_and_the_fft_code}

\lstinputlisting[breaklines]{../AA.f90}
\vspace{10pt}

\subsection{Aliasing, spectral leakage and the FFT plotting code}
\label{subsec:aliasing_spectral_leakage_and_the_fft_plotting_code}

\lstinputlisting[breaklines]{../AA.gp}
\vspace{10pt}

\subsection{Noise and the FFT code}
\label{subsec:noise_and_the_fft_code}

\lstinputlisting[breaklines]{../Noise_and_FFT.f90}
\vspace{10pt}

\subsection{Random numbers module}
\label{subsec:random_numbers_module}

\lstinputlisting[breaklines]{../../Modules/Random_numbers_module.f90}
\vspace{10pt}

\subsection{Dr Ouyed's random numbers module}
\label{subsec:dr_ouyeds_random_numbers_module}

\lstinputlisting[breaklines]{../../Modules/Ouyed_random_number_module.f90}
\vspace{10pt}

\subsection{Noise and the FFT plotting code}
\label{subsec:noise_and_the_fft_plotting_code}

\lstinputlisting[breaklines]{../Noise_and_FFT.gp}
\vspace{10pt}

\subsection{Lorenz system code}
\label{subsec:lorenz_system_code}

\lstinputlisting[breaklines]{../Fourier_Lorenz_System.f90}
\vspace{10pt}

\subsection{ODE system module}
\label{subsec:ode_system_module}

\lstinputlisting[breaklines]{../ODE_system_module.f90}
\vspace{10pt}

\subsection{Differential equations module}
\label{subsec:differential_equations_module}

\lstinputlisting[breaklines]{../../Modules/Differential_equations_module.f90}
\vspace{10pt}

\subsection{Lorenz system plotting code}
\label{subsec:lorenz_system_plotting_code}

\lstinputlisting[breaklines]{../Fourier_Lorenz_System.gp}
\vspace{10pt}

\subsection{Image analysis code}
\label{subsec:image_analysis_code}

\lstinputlisting[breaklines]{../Image_analysis.f90}
\vspace{10pt}

\subsection{Useful stuff module}
\label{subsec:useful_stuff_module}

\lstinputlisting[breaklines]{../../Modules/Useful_stuff_module.f90}

\end{document}
